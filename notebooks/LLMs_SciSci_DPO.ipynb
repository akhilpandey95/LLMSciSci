{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4cc393b69f0f41e0924fe35a411d0edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_503b6506f8ba453a85752167819adaa0",
              "IPY_MODEL_c5465260b6334755b24a8017e4f539ce",
              "IPY_MODEL_54de078cf1e742c3846e52afe298c5df"
            ],
            "layout": "IPY_MODEL_bbe14e55767a477e99dacc35a4abef7f"
          }
        },
        "503b6506f8ba453a85752167819adaa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_353cd29237b04c5795dddf21a66f643d",
            "placeholder": "​",
            "style": "IPY_MODEL_505deeb266204eec8dab2d532bc212bf",
            "value": "Filter: 100%"
          }
        },
        "c5465260b6334755b24a8017e4f539ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_734a622240cf430db963a29be00d0b65",
            "max": 184,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb9187ea5b3841bda4db8aaa7242c0a4",
            "value": 184
          }
        },
        "54de078cf1e742c3846e52afe298c5df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a74447ac854f4fc397d6796b6affaa06",
            "placeholder": "​",
            "style": "IPY_MODEL_cf44985272b649dc94226638f167cf87",
            "value": " 184/184 [00:00&lt;00:00, 887.82 examples/s]"
          }
        },
        "bbe14e55767a477e99dacc35a4abef7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "353cd29237b04c5795dddf21a66f643d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "505deeb266204eec8dab2d532bc212bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "734a622240cf430db963a29be00d0b65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb9187ea5b3841bda4db8aaa7242c0a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a74447ac854f4fc397d6796b6affaa06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf44985272b649dc94226638f167cf87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ac5db6db076420ba73a2477ea09f443": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c26e7ca2bc9473889954a4c7e3d5e7e",
              "IPY_MODEL_4911111db1ff46a5ba8ea6079b4e52e2",
              "IPY_MODEL_a02456a101db46d99fe0bc0fc4fc0322"
            ],
            "layout": "IPY_MODEL_d181bd1a5ba34c81b5b7c37efa384d8f"
          }
        },
        "9c26e7ca2bc9473889954a4c7e3d5e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9d72d210ded4735b06b84bebe1f025e",
            "placeholder": "​",
            "style": "IPY_MODEL_bed96901152a4bdf94bda3f2a723135e",
            "value": "Filter: 100%"
          }
        },
        "4911111db1ff46a5ba8ea6079b4e52e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bddc69906dc740b4995fe76c5f4e43d0",
            "max": 46,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbe7809140e54179bc1d47b98d5271c2",
            "value": 46
          }
        },
        "a02456a101db46d99fe0bc0fc4fc0322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ad113ed420a4e798cde017944e5798c",
            "placeholder": "​",
            "style": "IPY_MODEL_b60d135df6be477dad0309d59bd04338",
            "value": " 46/46 [00:00&lt;00:00, 790.97 examples/s]"
          }
        },
        "d181bd1a5ba34c81b5b7c37efa384d8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9d72d210ded4735b06b84bebe1f025e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bed96901152a4bdf94bda3f2a723135e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bddc69906dc740b4995fe76c5f4e43d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbe7809140e54179bc1d47b98d5271c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ad113ed420a4e798cde017944e5798c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b60d135df6be477dad0309d59bd04338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efbe6f3ced4d4718aad0d58277817d0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78e67937ea644f26a98527fc1b4daadf",
              "IPY_MODEL_021cb3eea4ba4c0c88efc2c08eb013d0",
              "IPY_MODEL_198f468750684724a3e04061ecace97e"
            ],
            "layout": "IPY_MODEL_304488de0ca743f3a4215eb5b108d021"
          }
        },
        "78e67937ea644f26a98527fc1b4daadf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7665807ec064d7795cc456c7f0927f0",
            "placeholder": "​",
            "style": "IPY_MODEL_6ad6773107c747f19849e89b848501d4",
            "value": "Extracting prompt in train dataset: 100%"
          }
        },
        "021cb3eea4ba4c0c88efc2c08eb013d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8abd5b189034104a2c3c873623c15bd",
            "max": 183,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6427555c29024677827ad9f1e77ad0a8",
            "value": 183
          }
        },
        "198f468750684724a3e04061ecace97e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6e0794f32094553934df7c9a03f2b26",
            "placeholder": "​",
            "style": "IPY_MODEL_7044c683d0744cd2be0d1932599c0c42",
            "value": " 183/183 [00:00&lt;00:00, 4314.23 examples/s]"
          }
        },
        "304488de0ca743f3a4215eb5b108d021": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7665807ec064d7795cc456c7f0927f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ad6773107c747f19849e89b848501d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8abd5b189034104a2c3c873623c15bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6427555c29024677827ad9f1e77ad0a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6e0794f32094553934df7c9a03f2b26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7044c683d0744cd2be0d1932599c0c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3c881d0501c4e5194301d92ceaf48a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b66ccc5481b4fb0a2cf38c599f75902",
              "IPY_MODEL_f7f76102e0574fc19040da9f4d8f4dce",
              "IPY_MODEL_29c2211c24bb4432a63ba2b222b0f666"
            ],
            "layout": "IPY_MODEL_d3dee70691924ea78e03dcbc98406136"
          }
        },
        "6b66ccc5481b4fb0a2cf38c599f75902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3c19d59c0ff43beab596e7cc918c2eb",
            "placeholder": "​",
            "style": "IPY_MODEL_39419fba4c244ab1b13fa06a6f2b6785",
            "value": "Applying chat template to train dataset: 100%"
          }
        },
        "f7f76102e0574fc19040da9f4d8f4dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71249d35fd3e4dc8bcbab5f4d4980e6d",
            "max": 183,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6c4a55a92cf4ed98400adde820a01c3",
            "value": 183
          }
        },
        "29c2211c24bb4432a63ba2b222b0f666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42992185ee814518aa0bf9421ed56441",
            "placeholder": "​",
            "style": "IPY_MODEL_2684580110c5413d908f84fe791d0607",
            "value": " 183/183 [00:00&lt;00:00, 5243.38 examples/s]"
          }
        },
        "d3dee70691924ea78e03dcbc98406136": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3c19d59c0ff43beab596e7cc918c2eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39419fba4c244ab1b13fa06a6f2b6785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71249d35fd3e4dc8bcbab5f4d4980e6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6c4a55a92cf4ed98400adde820a01c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42992185ee814518aa0bf9421ed56441": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2684580110c5413d908f84fe791d0607": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "688b9b1900e74561b4d2d36f465f75d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6414ef3fc2b444aaafbb97dbfadec3e3",
              "IPY_MODEL_2222945f7eda48659491b69e4780dd8d",
              "IPY_MODEL_a885d6b57bdd41aaa8a2187fe8787470"
            ],
            "layout": "IPY_MODEL_6b316bcbab6a4eaa870ff47b92728dc7"
          }
        },
        "6414ef3fc2b444aaafbb97dbfadec3e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a08fb8fa67c149f7922aeada5837c90a",
            "placeholder": "​",
            "style": "IPY_MODEL_b511232773a14b7b9e99c3cbb1535baa",
            "value": "Tokenizing train dataset: 100%"
          }
        },
        "2222945f7eda48659491b69e4780dd8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e03fcc7fe09944bc84472f9227a68689",
            "max": 183,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58c1e9afd19042809eece5cc9c2f26ff",
            "value": 183
          }
        },
        "a885d6b57bdd41aaa8a2187fe8787470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ef027df766344c1b4551bfb6802d8d6",
            "placeholder": "​",
            "style": "IPY_MODEL_e9a3c8bc6a914d229b7574fc4951786a",
            "value": " 183/183 [00:00&lt;00:00, 459.55 examples/s]"
          }
        },
        "6b316bcbab6a4eaa870ff47b92728dc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a08fb8fa67c149f7922aeada5837c90a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b511232773a14b7b9e99c3cbb1535baa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e03fcc7fe09944bc84472f9227a68689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58c1e9afd19042809eece5cc9c2f26ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ef027df766344c1b4551bfb6802d8d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9a3c8bc6a914d229b7574fc4951786a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Large Language models for Scientometrics\n",
        "\n",
        "**Large Language Models:**\n",
        "\n",
        "The capabilities of Large Language Models (**LLM's**) to process data from different modalities and excel at different tasks ranging from information extraction, question and answering, math, coding, and recently reasoning simply shows the potential of this technology. Intuitively the complexities of training these models on different datasets/data mixes, opting different architectural choices, choosing different alignment strategies **[1]** seemingly could suggest picking a specific model for each task, but **LLM's** are geared towards being considered as general task solvers.\n",
        "\n",
        "For this hands-on session we are going to use the Reproducibility dataset from the paper <u>Laying Foundations to Quantify the \"Effort of Reproducibility\"</u> **[2]** to preference tune answers using the **Direct Preference Optimization(DPO)** algorithm. *DPO* unlike other reinforcement algorithms directly applies maximum likelihood on the preference dataset to perform implicit reward modeling. Ideally, similar to most RL algorithms we would be applying the same reward maximization via **KL** divergence constraint. Theoretically, *DPO* is RL free, and doing a simple classification on a given a dataset $D$ that includes **chosen** and **rejected** responses. Learn more about *DPO* from the original paper **[3]**.\n",
        "\n",
        "**References**(s):\n",
        "<br>\n",
        "**[1]** [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)\n",
        "<br>\n",
        "**[2]** [Laying Foundations to Quantify the “Effort of Reproducibility”](https://ieeexplore.ieee.org/abstract/document/10266070)\n",
        "<br>\n",
        "**[3]** [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290)\n",
        "\n",
        "**Other Resources**:\n",
        "<br>\n",
        "**[R-1]** [Direct Preference Optimization (DPO) for LLM Alignment (From Scratch)\n",
        "](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb)\n",
        "<br>\n",
        "**[R-2]** [Preference Tuning for Summarization using Synthetic Data\n",
        "](https://github.com/anyscale/templates/tree/1939a34a54a0efeb1e86917d1175d92b50f482e6/templates/fine-tune-llm_v2/end-to-end-examples/fine-tune-preference#step-2-fine-tuning)\n",
        "\n",
        "<img src=\"https://images.ctfassets.net/cnu0m8re1exe/sIyPeDxgpIluQqQWK8nhS/67004d28ebbce2ca1f654a7a0afd92b3/SciSci.png\" align=\"center\" width=400 height=500>\n",
        "\n",
        ">(Credit: Davide Bonazzi) from [*Discover Magazine*](https://www.discovermagazine.com/the-sciences/the-science-of-science)"
      ],
      "metadata": {
        "id": "zT6MyuXRlkNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Table of Contents**:\n",
        "- Setup\n",
        "- Prepare Preference Dataset for **Direct Preference Optimization(DPO)**\n",
        "- API & Local Models setup\n",
        "- Preference tuning via **Direct Preference Optimization(DPO)**"
      ],
      "metadata": {
        "id": "Zgo0dfZ4qFZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Setup"
      ],
      "metadata": {
        "id": "o3ymvPHaqC1G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "q3HcRP_zlTbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "769f6d39-28e6-4eb3-8d38-3873b37f5e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing latest transformers...\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing tiktoken...\n",
            "Installing outlines...\n",
            "Installing huggingface-trl...\n",
            "Installing flash-attention-2...\n",
            "Installing bitsandbytes...\n",
            "Installing openai...\n"
          ]
        }
      ],
      "source": [
        "# @title 1.1 Install necessary libraries\n",
        "\n",
        "# install outlines\n",
        "print(f\"Installing latest transformers...\")\n",
        "!pip install -q git+https://github.com/huggingface/transformers\n",
        "\n",
        "# install tiktoken\n",
        "print(f\"Installing tiktoken...\")\n",
        "!pip install -q tiktoken\n",
        "\n",
        "# install outlines\n",
        "print(f\"Installing outlines...\")\n",
        "!pip install -q outlines\n",
        "\n",
        "# install huggingface-trl\n",
        "print(f\"Installing huggingface-trl...\")\n",
        "!pip install -q trl\n",
        "\n",
        "# install flash attention\n",
        "print(f\"Installing flash-attention-2...\")\n",
        "!pip install -q flash-attn --no-build-isolation\n",
        "\n",
        "# install bitsandbytes\n",
        "print(f\"Installing bitsandbytes...\")\n",
        "!pip install -q -U bitsandbytes\n",
        "\n",
        "# install openai\n",
        "print(f\"Installing openai...\")\n",
        "!pip install -q openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1.2 Import necessary libraries\n",
        "# This Source Code Form is subject to the terms of the MIT\n",
        "# License. If a copy of the same was not distributed with this\n",
        "# file, You can obtain one at\n",
        "# https://github.com/Northwestern-CSSI/LLMSciSci/blob/main/LICENSE.\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import bs4\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "import urllib3\n",
        "import pathlib\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "import openai as oai\n",
        "import seaborn as sns\n",
        "from pprint import pprint\n",
        "from peft import LoraConfig\n",
        "from ast import literal_eval\n",
        "from pydantic import BaseModel\n",
        "import matplotlib.pyplot as plt\n",
        "from bs4 import BeautifulSoup as BS\n",
        "from collections import defaultdict\n",
        "from outlines import models, generate\n",
        "from typing import List, Optional, Union\n",
        "from datasets import Dataset, DatasetDict\n",
        "from collections import Counter, OrderedDict\n",
        "from transformers import BitsAndBytesConfig, set_seed\n",
        "from pydantic import BaseModel, create_model, RootModel\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    Gemma3ForCausalLM,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig,\n",
        "    set_seed\n",
        ")\n",
        "from trl import (\n",
        "    DPOConfig,\n",
        "    DPOTrainer,\n",
        "    ModelConfig,\n",
        "    ScriptArguments,\n",
        "    TrlParser,\n",
        "    get_kbit_device_map,\n",
        "    get_peft_config,\n",
        "    get_quantization_config,\n",
        ")\n",
        "from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE"
      ],
      "metadata": {
        "cellView": "form",
        "id": "m3YRd_l4qQut"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/akhilpandey95/LLMSciSci/blob/main/media/LLMSciSci_dataset.png?raw=true\" width=650 height=475>"
      ],
      "metadata": {
        "id": "6lYjeUijtWsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1.3 Load `ReScience` dataset - [Download Data](https://drive.google.com/drive/folders/1qLCC5ZiDWoRtMQyBTeMxPrPlJLxcVgMN?usp=sharing)\n",
        "!ls -lah ./drive/MyDrive/CSSI/Lecture\n",
        "\n",
        "# set the directory\n",
        "os.chdir(\"./drive/MyDrive/CSSI/Lecture\")\n",
        "\n",
        "# read rescience\n",
        "rescience = pl.read_csv(\"./data/ReScience_JCDL-23.csv\")\n",
        "\n",
        "# show shape and columns\n",
        "print(\"-------------------------------\")\n",
        "print(f\"Data shape: {rescience.shape}\")\n",
        "print(\"-------------------------------\")\n",
        "\n",
        "\"\"\"\n",
        "Data columns: ['author', 'title', 'doi', 'article_type', 'lang', 'pdf_url', 'keywords', 'review_url',\n",
        "              'code_url', 'volume', 'issue', 'year', 'abstract', 'easy', 'difficult', 'gs_citations',\n",
        "              'gs_scholar_url', 'original_pdf_url', 'original_article_url', 'reason_for_easiness',\n",
        "              'reason_for_difficulty', 'limitations_results', 'scope_of_reproducibility',\n",
        "              'original_abstract', 'orig_art_sciparse_full_text', 'orig_art_pdfminer_full_text',\n",
        "              'original_sections', 'no_hyp', 'no_alg', 'no_images', 'no_equations', 'no_tables',\n",
        "              'is_meth_pres', 'is_intro_pres', 'link_to_code_available', 'mean_readability',\n",
        "              'hyp_available_in_text', 'easiness_longform', 'difficult_longform',\n",
        "              'list_for_limitations', 'list_for_diff', 'list_for_easiness', 'more_than_one_easy']\n",
        "\"\"\"\n",
        "\n",
        "# metadata\n",
        "meta_data_columns = [\"doi\", \"title\", \"review_url\", \"easy\", \"difficult\",\n",
        "                     \"scope_of_reproducibility\", \"reason_for_easiness\", \"reason_for_difficulty\"]\n",
        "print(f\"Rescience Metadata columns of interest: {meta_data_columns}\")\n",
        "print(\"-------------------------------\")\n",
        "\n",
        "# sneak peak of the data\n",
        "print(rescience.select(meta_data_columns).head())"
      ],
      "metadata": {
        "cellView": "form",
        "id": "e4HoEqRGtaUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72031895-bb99-48a9-a09d-64e5d0f45c8b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 88K\n",
            "drwx------  2 root root 4.0K Feb 28 17:35 data\n",
            "drwx------  4 root root 4.0K Mar 25 16:34 llmscisci-DPO-hinge-alpha\n",
            "drwx------  4 root root 4.0K Mar 25 17:12 llmscisci-DPO-hinge-beta_0.05\n",
            "drwx------  4 root root 4.0K Mar 25 16:53 llmscisci-DPO-hinge-beta_0.2\n",
            "drwx------  4 root root 4.0K Mar 25 17:03 llmscisci-DPO-hinge-beta_0.3\n",
            "drwx------  2 root root 4.0K Mar 25 04:07 llmscisci-DPO-rob_ep_0.1-alpha\n",
            "drwx------  2 root root 4.0K Mar 25 04:12 llmscisci-DPO-rob_ep_0.1-beta\n",
            "drwx------  2 root root 4.0K Mar 25 04:17 llmscisci-DPO-rob_ep_0.1-gamma\n",
            "drwx------  3 root root 4.0K Mar 25 14:55 llmscisci-DPO-rob_ep_0.1-tmp\n",
            "drwx------  4 root root 4.0K Mar 25 15:59 llmscisci-DPO-rob_ep_0.2-tmp\n",
            "drwx------  4 root root 4.0K Mar 25 16:16 llmscisci-DPO-rob_ep_0.3-tmp\n",
            "drwx------  2 root root 4.0K Mar 25 05:20 llmscisci-DPO-sigmoid-best\n",
            "drwx------  4 root root 4.0K Mar 25 17:33 llmscisci-DPO-sigmoid-beta-0.05\n",
            "drwx------  4 root root 4.0K Mar 25 17:42 llmscisci-DPO-sigmoid-beta-0.1\n",
            "drwx------  4 root root 4.0K Mar 25 17:51 llmscisci-DPO-sigmoid-beta-0.2\n",
            "drwx------  4 root root 4.0K Mar 25 18:00 llmscisci-DPO-sigmoid-beta-0.3\n",
            "drwx------  2 root root 4.0K Mar 25 03:36 llmscisci-DPO-WPO-alpha\n",
            "drwx------  2 root root 4.0K Mar 25 03:43 llmscisci-DPO-WPO-beta\n",
            "drwx------  2 root root 4.0K Mar 25 03:49 llmscisci-DPO-WPO-gamma\n",
            "drwx------  2 root root 4.0K Mar 24 01:03 media\n",
            "drwx------ 10 root root 4.0K Mar 25 13:24 models\n",
            "drwx------ 25 root root 4.0K Mar 25 17:55 wandb\n",
            "-------------------------------\n",
            "Data shape: (70, 43)\n",
            "-------------------------------\n",
            "Rescience Metadata columns of interest: ['doi', 'title', 'review_url', 'easy', 'difficult', 'scope_of_reproducibility', 'reason_for_easiness', 'reason_for_difficulty']\n",
            "-------------------------------\n",
            "shape: (5, 8)\n",
            "┌────────────┬────────────┬────────────┬───────────┬───────────┬───────────┬───────────┬───────────┐\n",
            "│ doi        ┆ title      ┆ review_url ┆ easy      ┆ difficult ┆ scope_of_ ┆ reason_fo ┆ reason_fo │\n",
            "│ ---        ┆ ---        ┆ ---        ┆ ---       ┆ ---       ┆ reproduci ┆ r_easines ┆ r_difficu │\n",
            "│ str        ┆ str        ┆ str        ┆ str       ┆ str       ┆ bility    ┆ s         ┆ lty       │\n",
            "│            ┆            ┆            ┆           ┆           ┆ ---       ┆ ---       ┆ ---       │\n",
            "│            ┆            ┆            ┆           ┆           ┆ i64       ┆ str       ┆ str       │\n",
            "╞════════════╪════════════╪════════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
            "│ 10.5281/ze ┆ [Re] Count ┆ https://op ┆ Overall,  ┆ Some expe ┆ 1         ┆ 1, 2, 4   ┆ 1, 2      │\n",
            "│ nodo.65746 ┆ erfactual  ┆ enreview.n ┆ clear env ┆ rimental  ┆           ┆           ┆           │\n",
            "│ 25         ┆ Generative ┆ et/forum?i ┆ ironment  ┆ details   ┆           ┆           ┆           │\n",
            "│            ┆ …          ┆ …          ┆ set…      ┆ were…     ┆           ┆           ┆           │\n",
            "│ 10.5281/ze ┆ [Re] Does  ┆ https://op ┆ The paper ┆ Since the ┆ 2         ┆ 1, 3      ┆ 1, 2      │\n",
            "│ nodo.65746 ┆ Self-Super ┆ enreview.n ┆ was well  ┆ codebase  ┆           ┆           ┆           │\n",
            "│ 29         ┆ vision     ┆ et/forum?i ┆ written   ┆ was       ┆           ┆           ┆           │\n",
            "│            ┆ Alw…       ┆ …          ┆ and…      ┆ incompl…  ┆           ┆           ┆           │\n",
            "│ 10.5281/ze ┆ [Re] Weakl ┆ https://op ┆           ┆           ┆ 3         ┆ 1,2,3     ┆ 3         │\n",
            "│ nodo.65746 ┆ y-Supervis ┆ enreview.n ┆ We found  ┆ The main  ┆           ┆           ┆           │\n",
            "│ 31         ┆ ed         ┆ et/forum?i ┆ particula ┆ difficult ┆           ┆           ┆           │\n",
            "│            ┆ Semanti…   ┆ …          ┆ rly easy  ┆ y of      ┆           ┆           ┆           │\n",
            "│            ┆            ┆            ┆ to…       ┆ replic…   ┆           ┆           ┆           │\n",
            "│ 10.5281/ze ┆ [Re] Repro ┆ https://op ┆ The       ┆ Some of   ┆ 1         ┆ 1,2,4     ┆ 2,4       │\n",
            "│ nodo.65746 ┆ ducibility ┆ enreview.n ┆ original  ┆ the exper ┆           ┆           ┆           │\n",
            "│ 35         ┆ Study of … ┆ et/forum?i ┆ paper     ┆ iments    ┆           ┆           ┆           │\n",
            "│            ┆            ┆ …          ┆ provides  ┆ requir…   ┆           ┆           ┆           │\n",
            "│            ┆            ┆            ┆ an…       ┆           ┆           ┆           ┆           │\n",
            "│ 10.5281/ze ┆ [Re] Data- ┆ https://op ┆ The simul ┆ To be     ┆ 2         ┆ 1, 2      ┆ 2         │\n",
            "│ nodo.65746 ┆ Driven     ┆ enreview.n ┆ ation     ┆ able to   ┆           ┆           ┆           │\n",
            "│ 37         ┆ Methods    ┆ et/forum?i ┆ logic as  ┆ run the   ┆           ┆           ┆           │\n",
            "│            ┆ for B…     ┆ …          ┆ well a…   ┆ authors’… ┆           ┆           ┆           │\n",
            "└────────────┴────────────┴────────────┴───────────┴───────────┴───────────┴───────────┴───────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Prepare Preference Dataset for **Direct Preference Optimization(DPO)**\n",
        "\n",
        "<img src=\"https://github.com/akhilpandey95/LLMSciSci/blob/main/media/LLMSciSci_DPO_dataset.png?raw=true\" width=700 height=450>"
      ],
      "metadata": {
        "id": "36-emY-kZeEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2.1 Load raw preference data from `GPT`, `Gemini` and `Llama` responses\n",
        "# read the gemini labelling data\n",
        "gemini_effortly = pl.read_csv(\"./data/gemini_effortly_labels_gamma.csv\")\n",
        "\n",
        "# read the gpt labelling data\n",
        "gpt_effortly = pl.read_csv(\"./data/gpt4_effortly_labels_beta.csv\")\n",
        "\n",
        "# read the llama labelling data\n",
        "llama_effortly = pl.read_csv(\"./data/llama3_effortly_labels_beta.csv\")\n",
        "\n",
        "# show a preview of the response(s)\n",
        "print(\"---------------------------\")\n",
        "print(f\"Response from gemini\")\n",
        "print(gemini_effortly.select(\"easy_gemini_response\")[0].item())\n",
        "print(\"---------------------------\")\n",
        "print(f\"Response from gpt\")\n",
        "print(gpt_effortly.select(\"easy_gpt_response\")[0].item())\n",
        "print(\"---------------------------\")\n",
        "print(f\"Response from llama\")\n",
        "print(llama_effortly.select(\"easy_llama3_response\")[0].item())\n",
        "print(\"---------------------------\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zUx1SwX5ppwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32c8ce79-9697-4e4c-8ad4-0b685933be2d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------\n",
            "Response from gemini\n",
            "The classification for the given description is:\n",
            "\n",
            "**[\"1. Availability of Code\", \"3. Readability of Full Text\"]**\n",
            "\n",
            "Here's why:\n",
            "\n",
            "* **1. Availability of Code:** The description explicitly states that the \"main components of the LiftedGAN was open-source, and implemented in PyTorch.\" This indicates the code was readily available, facilitating reproduction.\n",
            "* **3. Readability of Full Text:** The description mentions the paper being \"well-written.\" This implies the text was clear and understandable, making it easier to follow the methodology and understand the experiment. \n",
            "\n",
            "The description doesn't mention anything specific about supporting artifacts or the experimental setup, so those categories are not applicable. \n",
            "\n",
            "---------------------------\n",
            "Response from gpt\n",
            "{\n",
            "    \"1. Availability of Code\": \"Yes\",\n",
            "    \"2. Supporting Artifacts\": \"No\",\n",
            "    \"3. Readability of Full Text\": \"Yes\",\n",
            "    \"4. Experimental Setup or Environment\": \"No\",\n",
            "    \"5. Cannot extract concrete factors that Eased Reproducibility\": \"No\"\n",
            "}\n",
            "---------------------------\n",
            "Response from llama\n",
            "{\"1. Availability of Code\": \"Yes\", \"3. Readability of Full Text\": \"Yes\"}\n",
            "---------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2.2 helper class to build the $D_{ReproEffortDataset}$ preference dataset\n",
        "\n",
        "# helper function to load/initalize the prompt\n",
        "def process_prompt(raw_text, tokenizer, device, task=\"easy\", prompt_type=\"prompt\"):\n",
        "    \"\"\"\n",
        "    Given raw input text generate a prompt that will\n",
        "    be supplied to a preference dataset loader.\n",
        "\n",
        "    Parameters\n",
        "    ------------\n",
        "    arg1 | raw_text: str\n",
        "        Raw input text without prompt template\n",
        "    arg2 | tokenizer: transformers.tokenization_utils_fast.PreTrainedTokenizerFast\n",
        "        Tokenizer from the model\n",
        "    arg3 | device: str\n",
        "        Device name for the inputs and attention masks to sit on\n",
        "    arg4 | task: str[OPTIONAL]\n",
        "        Task type \"What was easy ?\" or \"What was difficult ?\"\n",
        "    arg5 | prompt_type: str[OPTIONAL]\n",
        "        String flag to be applied at the top of messages to create \"prompt\"\n",
        "        \"chosen\" or \"rejected\" chat responses for the preference dataset\n",
        "\n",
        "    Returns\n",
        "    ------------\n",
        "        Text\n",
        "    \"\"\"\n",
        "    # init\n",
        "    prompt = None\n",
        "    messages = []\n",
        "    add_generation_prompt = True\n",
        "    sys_prompt, user_prompt, input_text = None, None, None\n",
        "\n",
        "    # init system prompt available\n",
        "    sys_prompt = \"\"\"\n",
        "    You are a research assistant working on understanding the\n",
        "    spectrum of outputs researchers outline when reproducing\n",
        "    academic articles.\n",
        "    \"\"\"\n",
        "\n",
        "    # what was easy ?\n",
        "    if task == \"easy\":\n",
        "      # init user prompt for the task\n",
        "      user_prompt=\"\"\"\n",
        "      **Task:** You are given brief descriptions that made it easy for researcher\n",
        "      to reproduce original articles. Your goal is to analyze the brief description\n",
        "      and classify them into one or more from the following five categories,\n",
        "      which include:\n",
        "\n",
        "      1. Availability of Code\n",
        "      2. Supporting Artifacts\n",
        "      3. Readability of Full Text\n",
        "      4. Experimental Setup or Environment\n",
        "      5. Cannot extract concrete factors that Eased Reproducibility.\n",
        "      \"\"\"\n",
        "\n",
        "      # init the prompt\n",
        "      input_text = \"\"\"\n",
        "      **What was easy:**\n",
        "      ```plaintext\n",
        "      EASY_DESCRIPTION\n",
        "      ```\n",
        "      \"\"\"\n",
        "    else:\n",
        "      # init user prompt for the task\n",
        "      user_prompt=\"\"\"\n",
        "      **Task:** You are given brief descriptions that made it difficult for\n",
        "      researcher to reproduce original articles. Your goal is to analyze the\n",
        "      description and classify them into one or more from the following\n",
        "      five categories:\n",
        "\n",
        "      1. Missing Algorithm step or Architecture details\n",
        "      2. Missing nuance details\n",
        "      3. Unclear notation or documentation in the codebase\n",
        "      4. Insufficient Math/Equations\n",
        "      5. Cannot extract concrete factors that made it difficult for reproducibility.\n",
        "      \"\"\"\n",
        "\n",
        "      # init the prompt\n",
        "      input_text = \"\"\"\n",
        "      **What was difficult:**\n",
        "      ```\n",
        "      DIFF_DESCRIPTION\n",
        "      ```\n",
        "      \"\"\"\n",
        "\n",
        "    # apply chat template on the chosen/rejected response\n",
        "    if prompt_type == \"chosen\":\n",
        "      # set the chosen response for the preferences\n",
        "      messages.append([{\"role\": \"assistant\", \"content\": raw_text}])\n",
        "\n",
        "      # apply prompt template\n",
        "      add_generation_prompt=False\n",
        "\n",
        "      # apply prompt and remove the system prompt\n",
        "      prompt = tokenizer.apply_chat_template(messages, \\\n",
        "                                            tokenize=False, \\\n",
        "                                            use_system_prompt=add_generation_prompt, \\\n",
        "                                            add_generation_prompt=add_generation_prompt)\n",
        "    elif prompt_type == \"rejected\":\n",
        "      # set the rejected response for the preferences\n",
        "      messages.append([{\"role\": \"assistant\", \"content\": raw_text}])\n",
        "\n",
        "      # apply prompt template\n",
        "      add_generation_prompt=False\n",
        "\n",
        "      # apply prompt and remove the system prompt\n",
        "      prompt = tokenizer.apply_chat_template(messages, \\\n",
        "                                            tokenize=False, \\\n",
        "                                            use_system_prompt=add_generation_prompt, \\\n",
        "                                            add_generation_prompt=add_generation_prompt)\n",
        "    else:\n",
        "      # adjust and replace EASY_DESCRIPTION or DIFF_DESCRIPTION based on task\n",
        "      if task == \"easy\":\n",
        "        input_text = input_text.replace(\"EASY_DESCRIPTION\", raw_text)\n",
        "      else:\n",
        "        input_text = input_text.replace(\"DIFF_DESCRIPTION\", raw_text)\n",
        "\n",
        "      # set the prompt for the preferences\n",
        "      messages.append([\n",
        "          {\"role\": \"system\", \"content\": sys_prompt},\n",
        "          {\"role\": \"user\", \"content\": user_prompt + input_text}\n",
        "      ])\n",
        "\n",
        "      # apply prompt template\n",
        "      prompt = tokenizer.apply_chat_template(messages, \\\n",
        "                                            tokenize=False, \\\n",
        "                                            use_system_prompt=add_generation_prompt, \\\n",
        "                                            add_generation_prompt=add_generation_prompt)\n",
        "\n",
        "    # return the processed prompt\n",
        "    return prompt\n",
        "\n",
        "# utility class to create the preference dataset\n",
        "class ReproEffortPrefDataset:\n",
        "    def __init__(self, raw_data, tokenizer, device=\"cpu\"):\n",
        "        \"\"\"\n",
        "        Given raw text prepare preference dataset.\n",
        "\n",
        "        Parameters\n",
        "        ------------\n",
        "        arg1 | raw_data: polars.DataFrame or pandas.DataFrame or List[dict]\n",
        "            ML reproducibility challenge data processed with the following columns:\n",
        "              - \"easy\": Raw prompt text for the easy task.\n",
        "              - \"easy_gpt_response\": Chosen response for the easy task.\n",
        "              - \"easy_llama3_response\": Rejected response for the easy task.\n",
        "              - \"difficult\": Raw prompt text for the difficult task.\n",
        "              - \"difficult_gpt_response\": Chosen response for the difficult task.\n",
        "              - \"diff_llama3_response\": Rejected response for the difficult task.\n",
        "        arg2 | tokenizer: transformers.tokenization_utils_fast.PreTrainedTokenizerFast\n",
        "            Tokenizer from the model to apply chat template.\n",
        "        arg3 | device: str[OPTIONAL]\n",
        "            Device name (e.g., \"cpu\" or \"cuda\") for processing.\n",
        "\n",
        "        Returns\n",
        "        ------------\n",
        "            Text\n",
        "        \"\"\"\n",
        "        # polars df ? convert it to a pandas DataFrame.\n",
        "        if hasattr(raw_data, \"to_pandas\"):\n",
        "            raw_data = raw_data.to_pandas()\n",
        "        # pd df ? convert to list of dicts\n",
        "        if isinstance(raw_data, pd.DataFrame):\n",
        "            self.raw_data = raw_data.to_dict(\"records\")\n",
        "        elif isinstance(raw_data, list):\n",
        "            self.raw_data = raw_data\n",
        "        else:\n",
        "            raise ValueError(\"ERR[ReproEffortPrefDataset]: Unsupported raw_data type; must be a pl.DataFrame, pd.DataFrame, or list[dict].\")\n",
        "\n",
        "        # init default arguments\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "\n",
        "    # helper function to build the dataset object\n",
        "    def build_dataset(self, test_size=0.2, seed=2025):\n",
        "        \"\"\"\n",
        "        Build a unified preference dataset with\n",
        "        \"What was easy ?\" and \"What was difficult ?\" texts.\n",
        "\n",
        "        Parameters\n",
        "        ------------\n",
        "        arg1 | test_size: float[OPTIONAL]\n",
        "            Set the size of the test set, defaults to 0.2 or 20% of the dataset.\n",
        "        arg2 | seed: int[OPTIONAL]\n",
        "            Seed parameter for reproducibility, defaults to 2025.\n",
        "\n",
        "        Returns\n",
        "        ------------\n",
        "            Dictionary {\"prompt\": str, \"chosen\": str, \"rejected\": str}\n",
        "        \"\"\"\n",
        "        # inti list to store results\n",
        "        records = []\n",
        "\n",
        "        # set the keys for processing\n",
        "        task_types = [\"easy\", \"difficult\"]\n",
        "        chosen_types = [\"easy_gpt_response\", \"diff_gpt_response\"]\n",
        "        # rejected_types = [\"easy_llama3_response\", \"diff_llama3_response\"]\n",
        "        rejected_types = [\"easy_gemini_response\", \"diff_gemini_response\"]\n",
        "\n",
        "        # iterate and combine \"easy\" and \"difficult\" tasks\n",
        "        for sample in self.raw_data:\n",
        "            # procdess for each task\n",
        "            for idx, task in enumerate(task_types):\n",
        "                # set the prompt\n",
        "                prompt = process_prompt(sample[task], self.tokenizer, self.device, task=task, prompt_type=\"prompt\")[0]\n",
        "\n",
        "                # set the choosen response\n",
        "                chosen = process_prompt(sample[chosen_types[idx]], self.tokenizer, self.device, task=task, prompt_type=\"chosen\")[0]\n",
        "\n",
        "                # set the rejected response\n",
        "                rejected = process_prompt(sample[rejected_types[idx]], self.tokenizer, self.device, task=task, prompt_type=\"rejected\")[0]\n",
        "\n",
        "                # append the records\n",
        "                records.append({\n",
        "                    \"prompt\": prompt,\n",
        "                    \"chosen\": chosen,\n",
        "                    \"rejected\": rejected\n",
        "                })\n",
        "\n",
        "        # merge records\n",
        "        combined_data = {\n",
        "            \"prompt\": [r[\"prompt\"] for r in records],\n",
        "            \"chosen\": [r[\"chosen\"] for r in records],\n",
        "            \"rejected\": [r[\"rejected\"] for r in records],\n",
        "        }\n",
        "\n",
        "        # init hf dataset and perform train/test split.\n",
        "        dataset = Dataset.from_dict(combined_data)\n",
        "\n",
        "        # shuffle the datset\n",
        "        dataset = dataset.shuffle()\n",
        "\n",
        "        # train test split\n",
        "        dataset_split = dataset.train_test_split(test_size=test_size, seed=seed)\n",
        "\n",
        "        # return final dataset\n",
        "        return dataset_split"
      ],
      "metadata": {
        "cellView": "form",
        "id": "s6jdqCqzZieL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. API & Local Models setup\n",
        "\n",
        "For the commercial models you would need to setup your account and obtainan API key to run some of the experiments in this notebook.\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Pre-requisites for commercial models**\n",
        "<br>\n",
        "**OpenAI**: https://platform.openai.com/settings/organization/api-keys\n",
        "<hr>\n",
        "\n",
        "**Pre-requisites for local models**\n",
        "<br>\n",
        "The experiments and widgets in the notebook require `data/` and `models/`. Since `data/` is loaded, we need model weights which can be downloaded here:\n",
        "- [Models](https://drive.google.com/drive/folders/1aNT1SNA7Lz9kMgt5p1yGWST1T6D2Dmbd?usp=sharing)"
      ],
      "metadata": {
        "id": "M5rVre_oA4MD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3.1 Local model Catalog\n",
        "!ls -lah models/"
      ],
      "metadata": {
        "id": "c4mPh5ueBANa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "452b5778-5134-46df-a874-c6fecea4fecb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 32K\n",
            "drwx------ 2 root root 4.0K Feb 28 22:35 DeepSeek-R1-Distill-Llama-8B\n",
            "drwx------ 2 root root 4.0K Feb 28 22:04 DeepSeek-R1-Distill-Qwen-1.5B\n",
            "drwx------ 3 root root 4.0K Mar 25 13:24 gemma-3-1b-it\n",
            "drwx------ 2 root root 4.0K Mar 24 03:29 gemma-3-4b-it\n",
            "drwx------ 2 root root 4.0K Feb 28 22:04 Llama3.2-1B-Instruct\n",
            "drwx------ 2 root root 4.0K Feb 28 22:34 Llama3.2-3B-Instruct\n",
            "drwx------ 2 root root 4.0K Feb 28 22:05 Meta-Llama-3.1-8B-Instruct\n",
            "drwx------ 2 root root 4.0K Feb 28 22:26 Qwen2.5-1.5B-Instruct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3.2 Load model client or model-tokenizer pair\n",
        "# helper function to load/initalize the model\n",
        "def load_model(model_name, device):\n",
        "    \"\"\"\n",
        "    Given a model path, load tokenizer-model\n",
        "    pair and return the objects tagged to the\n",
        "    given device (cpu/cuda)\n",
        "\n",
        "    Parameters\n",
        "    ------------\n",
        "    arg1 | model_name: str\n",
        "        Use model catalog to load local model weights\n",
        "    arg2 | device: str\n",
        "        Hardware acceleration, defaults to \"cpu\" if any errors arise\n",
        "\n",
        "    Returns\n",
        "    ------------\n",
        "        Tuple(AutoModel, AutoTokenizer) for local (model_client, model_name)\n",
        "    \"\"\"\n",
        "    # device for acceleration\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    elif torch.mps.is_available():\n",
        "        device = \"mps\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "\n",
        "    # local models\n",
        "    local_models = [\"llama3.2-1b\", \"llama3.2-3b\", \"llama3.1-8b\", \"qwen2.5-1.5b\", \"r1-distill-qwen-1.5b\"]\n",
        "\n",
        "    # pathlib for models\n",
        "    model_path = pathlib.Path(\"/content/drive/MyDrive/CSSI/Lecture\")\n",
        "\n",
        "    # set the model-id\n",
        "    model_catalog = {\n",
        "        \"llama-3.2-1b\": model_path/f\"models/Llama3.2-1B-Instruct/\",\n",
        "        \"llama-3.2-3b\": model_path/f\"models/Llama3.2-3B-Instruct/hf/\",\n",
        "        \"llama-3.1-8b\": model_path/f\"models/Meta-Llama-3.1-8B-Instruct/hf/\",\n",
        "        \"gemma-3-4b\": model_path/f\"models/gemma-3-4b-it/\",\n",
        "        \"qwen-2.5-1.5b\": model_path/f\"models/Qwen2.5-1.5B-Instruct/\"\n",
        "    }\n",
        "\n",
        "    # set a model-id\n",
        "    model_id = model_catalog[model_name]\n",
        "\n",
        "    # log\n",
        "    print(\"----------------------------------\")\n",
        "    print(f\"Using {device} to load {model_id}\")\n",
        "    print(\"----------------------------------\")\n",
        "\n",
        "    # get model-tokenizer pair\n",
        "    start = time.time()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "    # based on model size switch quantization config\n",
        "    if model_name == \"llama3.1-70b\" or model_name == \"r1-distill-llama-70b\":\n",
        "        # 4-bit quantization config\n",
        "        bnb_4bit = BitsAndBytesConfig(\n",
        "          load_in_4bit=True,\n",
        "          bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "          bnb_4bit_quant_storage=torch.bfloat16\n",
        "        )\n",
        "\n",
        "        # 4 bit quantization\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_id, \\\n",
        "                                                   quantization_config=bnb_4bit, \\\n",
        "                                                   trust_remote_code=True, \\\n",
        "                                                   low_cpu_mem_usage=True, \\\n",
        "                                                   attn_implementation=\"sdpa\", \\\n",
        "                                                   device_map=device)\n",
        "    elif model_name == \"gemma-3-4b\":\n",
        "        model = Gemma3ForCausalLM.from_pretrained(model_id, \\\n",
        "                                              trust_remote_code=True, \\\n",
        "                                              torch_dtype=torch.bfloat16, \\\n",
        "                                              low_cpu_mem_usage=True, \\\n",
        "                                              attn_implementation=\"sdpa\", \\\n",
        "                                              device_map=device)\n",
        "    else:\n",
        "      # 4-bit quantization config\n",
        "      bnb_4bit = BitsAndBytesConfig(\n",
        "          load_in_4bit=True,\n",
        "          bnb_4bit_use_double_quant=True,\n",
        "          bnb_4bit_quant_type=\"nf4\",\n",
        "          bnb_4bit_compute_dtype=torch.bfloat16\n",
        "      )\n",
        "\n",
        "      # load bfloat16 weights\n",
        "      model = AutoModelForCausalLM.from_pretrained(model_id, \\\n",
        "                                                   trust_remote_code=True, \\\n",
        "                                                   torch_dtype=torch.bfloat16, \\\n",
        "                                                   low_cpu_mem_usage=True, \\\n",
        "                                                   attn_implementation=\"flash_attention_2\", \\\n",
        "                                                   device_map=device)\n",
        "\n",
        "    # is it a llama tokenizer ?\n",
        "    if \"llama\" in model_name:\n",
        "        # pad token if needed\n",
        "        tokenizer.add_special_tokens({\"pad_token\": \"<|finetune_right_pad_id|>\"})\n",
        "        print(f\"Setting <|finetune_right_pad_id|> token for {model_id}\")\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "        # llama prompt template\n",
        "        llama_template = r\"\"\"\n",
        "        {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
        "        \"\"\"\n",
        "\n",
        "        # set the chat template\n",
        "        tokenizer.chat_template = llama_template\n",
        "\n",
        "    # gemma eos token\n",
        "    if \"gemma\" in model_name:\n",
        "        # set the EOS tokenid\n",
        "        tokenizer.eos_token_id = tokenizer.encode(\"<end_of_turn>\")[0]\n",
        "\n",
        "    # load time\n",
        "    end = time.time()\n",
        "    print(f\"Model-tokenizer Load Time:, {end - start} seconds\")\n",
        "    print(\"----------------------------------\")\n",
        "\n",
        "    # return the pair\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pwQaMpcXBDrh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3.3 Load policy model for $\\pi_{LLMSciSci}$\n",
        "# load the model for policy update\n",
        "# model, tokenizer = load_model(\"qwen-2.5-1.5b\", \"cuda\")\n",
        "model, tokenizer = load_model(\"llama-3.2-1b\", \"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1WIFUjnTg-k",
        "outputId": "5df5b196-9ef7-4650-d0d6-24a7ab8123a5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------\n",
            "Using cuda to load /content/drive/MyDrive/CSSI/Lecture/models/Llama3.2-1B-Instruct\n",
            "----------------------------------\n",
            "Setting <|finetune_right_pad_id|> token for /content/drive/MyDrive/CSSI/Lecture/models/Llama3.2-1B-Instruct\n",
            "Model-tokenizer Load Time:, 3.2791781425476074 seconds\n",
            "----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Preference tuning via **Direct Preference Optimization(DPO)**\n",
        "\n",
        "$$\n",
        "L_{DPO}(\\pi_{LLMSciSci}: \\pi_{LLM-instruct})\n",
        "\\;=\\; - \\,\\mathbb{E}{\\bigl(x,\\,r^+,\\,r^-\\bigr) \\sim D_{ReproEffortDataset}}\n",
        "\\Bigl[\n",
        "\\log \\,\\sigma\\!\\Bigl(\n",
        "r_\\theta(x,r^+) \\;-\\; r_\\theta(x,r^-)\n",
        "\\Bigr)\n",
        "\\Bigr]\n",
        "$$\n",
        "\n",
        "$$\n",
        "r_\\theta(x, r)\n",
        "\\;=\\;\n",
        "\\beta \\,\\log \\frac{\\pi_{LLMSciSci}(r \\,\\vert\\, x)}{\\pi_{LLM-instruct}(r \\,\\vert\\, x)}\n",
        "$$\n",
        "\n",
        "where the $r_{\\theta}$ is computed\n",
        "- using $r^+$(human preferred response), and $r^-$(rejected responses).\n",
        "- for the models $\\pi_{LLMSciSci}$ and $\\pi_{LLM-instruct}$.\n",
        "- $r_{\\theta}$  captures the log-probability of the *chosen* vs *rejected* responses on $D_{ReproEffortDataset}$.\n",
        "- $\\pi_{LLM-instruct}$ is the instruct-tuned open weight reference model.\n",
        "- $\\pi_{LLMSciSci}$ is the final RL model intended to be preference-tuned on $D_{ReproEffortDataset}$."
      ],
      "metadata": {
        "id": "Uy-zyRUbjg46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4.1 Init `ReproEffortPrefDataset` dataset object, train/test split\n",
        "\n",
        "# # combine the GPT and Llama datasets\n",
        "# raw_data = gpt_effortly.join(\n",
        "#     llama_effortly.select([\"doi\", \"easy\", \"difficult\", \"easy_llama3_response\", \"easy_llama3_label\", \"diff_llama3_response\", \"diff_llama3_label\"]),\n",
        "#     on=\"doi\",\n",
        "#     how=\"inner\"\n",
        "# )\n",
        "\n",
        "# combine the GPT and Gemini datasets\n",
        "raw_data = gpt_effortly.join(\n",
        "    gemini_effortly.select([\"doi\", \"easy\", \"difficult\", \"easy_gemini_response\", \"easy_gemini_label\", \"diff_gemini_response\", \"diff_gemini_label\"]),\n",
        "    on=\"doi\",\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "# final shape of the raw data\n",
        "print(f\"-----------------------------------\")\n",
        "print(f\"Shape of raw_data: {raw_data.shape}\")\n",
        "print(f\"-----------------------------------\")\n",
        "print(\"Columns in raw_data:\")\n",
        "print(f\"-----------------------------------\")\n",
        "pprint(raw_data.columns)\n",
        "print(f\"-----------------------------------\")\n",
        "\n",
        "# convert raw data to pandas\n",
        "raw_data_pd = raw_data.to_pandas()\n",
        "\n",
        "# create the preference dataset\n",
        "dataset_obj = ReproEffortPrefDataset(raw_data_pd, tokenizer, device=\"cuda\")\n",
        "dataset = dataset_obj.build_dataset(test_size=0.2, seed=2025)\n",
        "print(f\"-----------------------------------\")\n",
        "print(f\"ReproEffortPrefDataset:\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5PWOkOxBpM24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3d533bc-5678-4cc1-fc1c-c02d07185c5b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------\n",
            "Shape of raw_data: (115, 15)\n",
            "-----------------------------------\n",
            "Columns in raw_data:\n",
            "-----------------------------------\n",
            "['doi',\n",
            " 'easy',\n",
            " 'difficult',\n",
            " 'easy_gpt_prompt',\n",
            " 'diff_gpt_prompt',\n",
            " 'easy_gpt_response',\n",
            " 'diff_gpt_response',\n",
            " 'y_easy_gpt4',\n",
            " 'y_diff_gpt4',\n",
            " 'easy_right',\n",
            " 'difficult_right',\n",
            " 'easy_gemini_response',\n",
            " 'easy_gemini_label',\n",
            " 'diff_gemini_response',\n",
            " 'diff_gemini_label']\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "ReproEffortPrefDataset:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['prompt', 'chosen', 'rejected'],\n",
            "        num_rows: 184\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['prompt', 'chosen', 'rejected'],\n",
            "        num_rows: 46\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4.2 Preview of the preference dataset for \"What was easy ?\" and \"What was difficult ?\" tasks\n",
        "\n",
        "# print sample cut of the dataset\n",
        "print(\"-----------------------------------\")\n",
        "print(\"Prompt for the preference dataset..\")\n",
        "print(dataset[\"train\"][0][\"prompt\"])\n",
        "print(\"-----------------------------------\")\n",
        "print(\"CHOSEN response:\")\n",
        "print(dataset[\"train\"][0][\"chosen\"])\n",
        "print(\"-----------------------------------\")\n",
        "print(\"REJECTED response:\")\n",
        "print(dataset[\"train\"][0][\"rejected\"])\n",
        "print(\"-----------------------------------\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qd5DGcihsh_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93b5a73a-2279-4a57-f0d5-4de169964895"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------\n",
            "Prompt for the preference dataset..\n",
            "\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "You are a research assistant working on understanding the\n",
            "    spectrum of outputs researchers outline when reproducing\n",
            "    academic articles.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "**Task:** You are given brief descriptions that made it easy for researcher\n",
            "      to reproduce original articles. Your goal is to analyze the brief description\n",
            "      and classify them into one or more from the following five categories,\n",
            "      which include:\n",
            "\n",
            "      1. Availability of Code\n",
            "      2. Supporting Artifacts\n",
            "      3. Readability of Full Text\n",
            "      4. Experimental Setup or Environment\n",
            "      5. Cannot extract concrete factors that Eased Reproducibility.\n",
            "      \n",
            "      **What was easy:**\n",
            "      ```plaintext\n",
            "      The publicly available codebases were well documented and easy to follow. The authors have also mentioned sources for the processed datasets that they have used. The simu‐ lation data generation code for the imitation learning model was also shared.\n",
            "1https://github.com/vita‐epfl/social‐nce\n",
            "Copyright © 2022 R. Sen et al., released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Roopsa Sen (roopsa.sen@kgpian.iitkgp.ac.in) The authors have declared that no competing interests exist. Code is available at https://github.com/RoopsaSen/social-nce-trajectron-plus-plus – DOI 10.5281/zenodo.6511007. – SWH swh:1:dir:ba72ac2acf3bac942d9b3a66e51091e6bcce6617. Open peer review is available at https://openreview.net/forum?id=SIQEl6f7h0Y.\n",
            "ReScience C 8.2 (#36) – Sen et al. 2022 1\n",
            "      ```<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "        \n",
            "-----------------------------------\n",
            "CHOSEN response:\n",
            "\n",
            "<|begin_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "{\n",
            "  \"1. Availability of Code\": \"Yes\",\n",
            "  \"2. Supporting Artifacts\": \"Yes\",\n",
            "  \"3. Readability of Full Text\": \"No\",\n",
            "  \"4. Experimental Setup or Environment\": \"Yes\",\n",
            "  \"5. Cannot extract concrete factors that Eased Reproducibility\": \"No\"\n",
            "}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "        \n",
            "-----------------------------------\n",
            "REJECTED response:\n",
            "\n",
            "<|begin_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Here's the classification for the given description:\n",
            "\n",
            "**[\"1. Availability of Code\", \"2. Supporting Artifacts\"]**\n",
            "\n",
            "Here's why:\n",
            "\n",
            "* **1. Availability of Code:**  The text explicitly mentions \"publicly available codebases,\"  \"simulation data generation code,\" and provides specific GitHub links.\n",
            "* **2. Supporting Artifacts:**  The statement \"The authors have also mentioned sources for the processed datasets that they have used\" indicates the availability of data sources, which can be considered supporting artifacts.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "        \n",
            "-----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4.3 Tokenomics to decide `max_seq_length` and `prompt_length`\n",
        "# gather the train and test datasets\n",
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]\n",
        "\n",
        "# lets find the p95 length of the prompt\n",
        "prompt_length = int(np.percentile([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset[\"prompt\"]], 95))\n",
        "max_seq_length_chosen = int(np.percentile([len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) for x in train_dataset], 95))\n",
        "max_seq_length_rejected = int(np.percentile([len(tokenizer(x[\"prompt\"] + x[\"rejected\"])[\"input_ids\"]) for x in train_dataset], 95))\n",
        "max_seq_length = max(max_seq_length_chosen, max_seq_length_rejected)\n",
        "\n",
        "# filter datasets to remove samples that are too long\n",
        "train_dataset = train_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) <= max_seq_length)\n",
        "test_dataset = test_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) <= max_seq_length)\n",
        "print(f\"len(train_dataset): {len(train_dataset)}\")\n",
        "print(f\"len(test_dataset): {len(test_dataset)}\")\n",
        "\n",
        "# Up the lengths to next multiple of 2, why 2? Don't know\n",
        "prompt_length = ((prompt_length + 1) // 2) * 2\n",
        "max_seq_length = ((max_seq_length + 1) // 2) * 2\n",
        "print(f\"p95 prompt length: {prompt_length}\")\n",
        "print(f\"p95 prompt + chosen length: {max_seq_length}\")\n",
        "\n",
        "# prompt_length = 512\n",
        "# max_seq_length = 512"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DWEUbK_x16Bn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "4cc393b69f0f41e0924fe35a411d0edd",
            "503b6506f8ba453a85752167819adaa0",
            "c5465260b6334755b24a8017e4f539ce",
            "54de078cf1e742c3846e52afe298c5df",
            "bbe14e55767a477e99dacc35a4abef7f",
            "353cd29237b04c5795dddf21a66f643d",
            "505deeb266204eec8dab2d532bc212bf",
            "734a622240cf430db963a29be00d0b65",
            "cb9187ea5b3841bda4db8aaa7242c0a4",
            "a74447ac854f4fc397d6796b6affaa06",
            "cf44985272b649dc94226638f167cf87",
            "7ac5db6db076420ba73a2477ea09f443",
            "9c26e7ca2bc9473889954a4c7e3d5e7e",
            "4911111db1ff46a5ba8ea6079b4e52e2",
            "a02456a101db46d99fe0bc0fc4fc0322",
            "d181bd1a5ba34c81b5b7c37efa384d8f",
            "e9d72d210ded4735b06b84bebe1f025e",
            "bed96901152a4bdf94bda3f2a723135e",
            "bddc69906dc740b4995fe76c5f4e43d0",
            "fbe7809140e54179bc1d47b98d5271c2",
            "2ad113ed420a4e798cde017944e5798c",
            "b60d135df6be477dad0309d59bd04338"
          ]
        },
        "outputId": "c8af4538-a575-43ab-8bba-81fcc2089ba8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/184 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cc393b69f0f41e0924fe35a411d0edd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/46 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ac5db6db076420ba73a2477ea09f443"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(train_dataset): 183\n",
            "len(test_dataset): 46\n",
            "p95 prompt length: 390\n",
            "p95 prompt + chosen length: 586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4.4 Train the first $\\pi_{LLMSciSci}$ policy via $L_{DPO}$ using the $\\sigma$ loss\n",
        "\n",
        "# # LoRA config\n",
        "# peft_config = LoraConfig(\n",
        "#     lora_alpha=128,\n",
        "#     lora_dropout=0.05,\n",
        "#     r=256,\n",
        "#     bias=\"none\",\n",
        "#     target_modules=\"all-linear\",\n",
        "#     task_type=\"CAUSAL_LM\",\n",
        "# )\n",
        "\n",
        "# # dpo params\n",
        "# dpo_args = {\n",
        "#     \"beta\": 0.3,\n",
        "#     \"loss_type\": \"sigmoid\"\n",
        "# }\n",
        "\n",
        "# # args\n",
        "# training_args = DPOConfig(output_dir=\"llmscisci-DPO-sigmoid-beta-0.3\", \\\n",
        "#                           run_name=\"rn-llmscisci-DPO-sigmoid-beta-0.3\", \\\n",
        "#                           logging_steps=10, \\\n",
        "#                           num_train_epochs=10, \\\n",
        "#                           max_length=max_seq_length, \\\n",
        "#                           max_prompt_length=prompt_length, \\\n",
        "#                           beta=dpo_args[\"beta\"], \\\n",
        "#                           loss_type=dpo_args[\"loss_type\"], \\\n",
        "#                           label_names=[\"chosen\", \"rejected\"])\n",
        "\n",
        "# # init DPO trainer\n",
        "# trainer = DPOTrainer(model=model, \\\n",
        "#                      peft_config=peft_config, \\\n",
        "#                      args=training_args, \\\n",
        "#                      processing_class=tokenizer, \\\n",
        "#                      train_dataset=train_dataset)\n",
        "\n",
        "# # train\n",
        "# trainer.train()\n",
        "\n",
        "# # save model weights\n",
        "# trainer.save_model()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7EOED_aVNzus"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4.5 Train the first $\\pi_{LLMSciSci}$ policy via $L_{DPO}$ using the $WPO$ loss\n",
        "\n",
        "# # LoRA config\n",
        "# peft_config = LoraConfig(\n",
        "#     lora_alpha=128,\n",
        "#     lora_dropout=0.05,\n",
        "#     r=256,\n",
        "#     bias=\"none\",\n",
        "#     target_modules=\"all-linear\",\n",
        "#     task_type=\"CAUSAL_LM\",\n",
        "# )\n",
        "\n",
        "# # dpo params\n",
        "# dpo_args = {\n",
        "#     \"beta\": 0.1,                            # The beta factor in DPO loss. Higher beta means less divergence\n",
        "#     \"loss_type\": \"sigmoid\"                  # The loss type for DPO.\n",
        "# }\n",
        "\n",
        "# # args\n",
        "# training_args = DPOConfig(output_dir=\"llmscisci-DPO-WPO-gamma\", \\\n",
        "#                           run_name=\"rn-llmscisci-DPO-WPO-gamma\", \\\n",
        "#                           use_weighting=True, \\\n",
        "#                           logging_steps=10, \\\n",
        "#                           num_train_epochs=10, \\\n",
        "#                           max_length=max_seq_length, \\\n",
        "#                           max_prompt_length=prompt_length, \\\n",
        "#                           beta=dpo_args[\"beta\"], \\\n",
        "#                           loss_type=dpo_args[\"loss_type\"], \\\n",
        "#                           label_names=[\"chosen\", \"rejected\"])\n",
        "\n",
        "# # init DPO trainer\n",
        "# trainer = DPOTrainer(model=model, \\\n",
        "#                      peft_config=peft_config, \\\n",
        "#                      args=training_args, \\\n",
        "#                      processing_class=tokenizer, \\\n",
        "#                      train_dataset=train_dataset)\n",
        "\n",
        "# # train\n",
        "# trainer.train()\n",
        "\n",
        "# # save model weights\n",
        "# trainer.save_model()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Q1eUTMGGXLgq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4.6 Train the first $\\pi_{LLMSciSci}$ policy via $L_{DPO}$ using the $rDPO$ loss with $\\epsilon = 0.2$, label_smoothening=0.05\n",
        "\n",
        "# # LoRA config\n",
        "# peft_config = LoraConfig(\n",
        "#     lora_alpha=128,\n",
        "#     lora_dropout=0.05,\n",
        "#     r=256,\n",
        "#     bias=\"none\",\n",
        "#     target_modules=\"all-linear\",\n",
        "#     task_type=\"CAUSAL_LM\",\n",
        "# )\n",
        "\n",
        "# # dpo params\n",
        "# dpo_args = {\n",
        "#     \"beta\": 0.3,\n",
        "#     \"loss_type\": \"robust\"\n",
        "# }\n",
        "\n",
        "# # args\n",
        "# training_args = DPOConfig(output_dir=\"llmscisci-DPO-rob_ep_0.3-tmp\", \\\n",
        "#                           run_name=\"rn-llmscisci-DPO-rob_ep_0.3-tmp\", \\\n",
        "#                           label_smoothing=0.05, \\\n",
        "#                           logging_steps=10, \\\n",
        "#                           num_train_epochs=10, \\\n",
        "#                           max_length=max_seq_length, \\\n",
        "#                           max_prompt_length=prompt_length, \\\n",
        "#                           beta=dpo_args[\"beta\"], \\\n",
        "#                           loss_type=dpo_args[\"loss_type\"], \\\n",
        "#                           label_names=[\"chosen\", \"rejected\"])\n",
        "\n",
        "# # init DPO trainer\n",
        "# trainer = DPOTrainer(model=model, \\\n",
        "#                      peft_config=peft_config, \\\n",
        "#                      args=training_args, \\\n",
        "#                      processing_class=tokenizer, \\\n",
        "#                      train_dataset=train_dataset)\n",
        "\n",
        "# # train\n",
        "# trainer.train()\n",
        "\n",
        "# # save model weights\n",
        "# trainer.save_model()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZKTiekjuboc_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4.7 Train the first $\\pi_{LLMSciSci}$ policy via $L_{DPO}$ using the $hinge$ loss with $\\beta = 0.05$\n",
        "\n",
        "# LoRA config\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=128,\n",
        "    lora_dropout=0.05,\n",
        "    r=256,\n",
        "    bias=\"none\",\n",
        "    target_modules=\"all-linear\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# dpo params\n",
        "dpo_args = {\n",
        "    \"beta\": 0.05,\n",
        "    \"loss_type\": \"hinge\"\n",
        "}\n",
        "\n",
        "# args\n",
        "training_args = DPOConfig(output_dir=\"llmscisci-DPO-best\", \\\n",
        "                          run_name=\"rn-llmscisci-DPO-best\", \\\n",
        "                          logging_steps=10, \\\n",
        "                          num_train_epochs=10, \\\n",
        "                          max_length=max_seq_length, \\\n",
        "                          max_prompt_length=prompt_length, \\\n",
        "                          beta=dpo_args[\"beta\"], \\\n",
        "                          loss_type=dpo_args[\"loss_type\"], \\\n",
        "                          label_names=[\"chosen\", \"rejected\"])\n",
        "\n",
        "# init DPO trainer\n",
        "trainer = DPOTrainer(model=model, \\\n",
        "                     peft_config=peft_config, \\\n",
        "                     args=training_args, \\\n",
        "                     processing_class=tokenizer, \\\n",
        "                     train_dataset=train_dataset)\n",
        "\n",
        "# train\n",
        "trainer.train()\n",
        "\n",
        "# save model weights\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "efbe6f3ced4d4718aad0d58277817d0c",
            "78e67937ea644f26a98527fc1b4daadf",
            "021cb3eea4ba4c0c88efc2c08eb013d0",
            "198f468750684724a3e04061ecace97e",
            "304488de0ca743f3a4215eb5b108d021",
            "e7665807ec064d7795cc456c7f0927f0",
            "6ad6773107c747f19849e89b848501d4",
            "b8abd5b189034104a2c3c873623c15bd",
            "6427555c29024677827ad9f1e77ad0a8",
            "f6e0794f32094553934df7c9a03f2b26",
            "7044c683d0744cd2be0d1932599c0c42",
            "d3c881d0501c4e5194301d92ceaf48a5",
            "6b66ccc5481b4fb0a2cf38c599f75902",
            "f7f76102e0574fc19040da9f4d8f4dce",
            "29c2211c24bb4432a63ba2b222b0f666",
            "d3dee70691924ea78e03dcbc98406136",
            "d3c19d59c0ff43beab596e7cc918c2eb",
            "39419fba4c244ab1b13fa06a6f2b6785",
            "71249d35fd3e4dc8bcbab5f4d4980e6d",
            "d6c4a55a92cf4ed98400adde820a01c3",
            "42992185ee814518aa0bf9421ed56441",
            "2684580110c5413d908f84fe791d0607",
            "688b9b1900e74561b4d2d36f465f75d7",
            "6414ef3fc2b444aaafbb97dbfadec3e3",
            "2222945f7eda48659491b69e4780dd8d",
            "a885d6b57bdd41aaa8a2187fe8787470",
            "6b316bcbab6a4eaa870ff47b92728dc7",
            "a08fb8fa67c149f7922aeada5837c90a",
            "b511232773a14b7b9e99c3cbb1535baa",
            "e03fcc7fe09944bc84472f9227a68689",
            "58c1e9afd19042809eece5cc9c2f26ff",
            "9ef027df766344c1b4551bfb6802d8d6",
            "e9a3c8bc6a914d229b7574fc4951786a"
          ]
        },
        "cellView": "form",
        "id": "Fx7YnRTzHE0o",
        "outputId": "eba3540b-9aac-425f-e035-ca743eac488f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting prompt in train dataset:   0%|          | 0/183 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efbe6f3ced4d4718aad0d58277817d0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Applying chat template to train dataset:   0%|          | 0/183 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3c881d0501c4e5194301d92ceaf48a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/183 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "688b9b1900e74561b4d2d36f465f75d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makhilpandey095\u001b[0m (\u001b[33makhilpandey095-kellogg-school-center-for-nonprofit-manag\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/CSSI/Lecture/wandb/run-20250325_180512-h659p3qe</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/akhilpandey095-kellogg-school-center-for-nonprofit-manag/huggingface/runs/h659p3qe' target=\"_blank\">rn-llmscisci-DPO-best</a></strong> to <a href='https://wandb.ai/akhilpandey095-kellogg-school-center-for-nonprofit-manag/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/akhilpandey095-kellogg-school-center-for-nonprofit-manag/huggingface' target=\"_blank\">https://wandb.ai/akhilpandey095-kellogg-school-center-for-nonprofit-manag/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/akhilpandey095-kellogg-school-center-for-nonprofit-manag/huggingface/runs/h659p3qe' target=\"_blank\">https://wandb.ai/akhilpandey095-kellogg-school-center-for-nonprofit-manag/huggingface/runs/h659p3qe</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [230/230 04:57, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.954300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.792200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.538100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.241300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.095700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.013100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.009000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.006300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.009100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.001600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.002500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.005600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.002800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.009400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Inference check $\\pi_{LLMSciSci}$ against $\\pi_{LLM-instruct}$ on sample outputs"
      ],
      "metadata": {
        "id": "K9afKJETVwKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5.1 Load reference model for $\\pi_{LLM-Instruct}$\n",
        "# load reference model without policy update\n",
        "ref_model, _ = load_model(\"llama-3.2-1b\", \"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "KwkiJSp7WFHB",
        "outputId": "9e06a217-fbda-42d9-fb01-8d6b9ba8eff1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------\n",
            "Using cuda to load /content/drive/MyDrive/CSSI/Lecture/models/Llama3.2-1B-Instruct\n",
            "----------------------------------\n",
            "Setting <|finetune_right_pad_id|> token for /content/drive/MyDrive/CSSI/Lecture/models/Llama3.2-1B-Instruct\n",
            "Model-tokenizer Load Time:, 3.4579243659973145 seconds\n",
            "----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5.2 Sample inference check on $D_{test}$\n",
        "from IPython.display import display, Latex\n",
        "\n",
        "# seed for reproducibility\n",
        "set_seed(2025)\n",
        "\n",
        "# set top_p and temperature to none\n",
        "ref_model.generation_config.temperature=None\n",
        "ref_model.generation_config.top_p=None\n",
        "trainer.model.generation_config.temperature=None\n",
        "trainer.model.generation_config.top_p=None\n",
        "\n",
        "# inputs, attention mask, and shape\n",
        "idx = 0\n",
        "input_encoded = tokenizer(test_dataset[\"prompt\"][idx], padding=True, return_tensors=\"pt\")\n",
        "input_encoded_ids = input_encoded[\"input_ids\"].to(\"cuda\")\n",
        "input_encoded_attn_mask = input_encoded[\"attention_mask\"].to(\"cuda\")\n",
        "ref_input_encoded_ids = input_encoded[\"input_ids\"].to(\"cuda\")\n",
        "ref_input_encoded_attn_mask = input_encoded[\"attention_mask\"].to(\"cuda\")\n",
        "input_shape = len(input_encoded[\"input_ids\"][0])\n",
        "\n",
        "# model outputs on test prompts\n",
        "outputs = trainer.model.generate(\n",
        "    input_ids=input_encoded_ids,\n",
        "    attention_mask=input_encoded_attn_mask,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.encode(\"<|eot_id|>\")\n",
        ")\n",
        "\n",
        "# reference model outputs on test prompts\n",
        "ref_outputs = ref_model.generate(\n",
        "    input_ids=ref_input_encoded_ids,\n",
        "    attention_mask=ref_input_encoded_attn_mask,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.encode(\"<|eot_id|>\")\n",
        ")\n",
        "\n",
        "print(\"------------------------------------------------------\")\n",
        "# print(\"Input:\")\n",
        "# print(test_dataset[\"prompt\"][idx])\n",
        "# print(\"------------------------------------------------------\")\n",
        "print(\"Policy model response\")\n",
        "display(Latex(r'\\pi_{LLMSciSci}:'))\n",
        "output = tokenizer.decode(outputs[0][input_shape:], skip_special_tokens=True)\n",
        "print(output)\n",
        "print(\"------------------------------------------------------\")\n",
        "print(\"Reference model response (llama-3.2-1b)\")\n",
        "display(Latex(r'\\pi_{LLM-instruct}:'))\n",
        "ref_output = tokenizer.decode(ref_outputs[0][input_shape:], skip_special_tokens=True)\n",
        "print(ref_output)\n",
        "print(\"------------------------------------------------------\")\n",
        "print(\"Correct response:\")\n",
        "print(test_dataset[\"chosen\"][idx])\n",
        "print(\"------------------------------------------------------\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gX5dFJ3OjcIM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "outputId": "c9685f6c-b3bb-4549-e0ee-54b91bc5de14"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------\n",
            "Policy model response\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Latex object>"
            ],
            "text/latex": "\\pi_{LLMSciSci}:"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I would classify the given brief description into the following categories:\n",
            "\n",
            "1. **Availability of Code**: Yes\n",
            "2. **Supporting Artifacts**: Yes\n",
            "3. **Readability of Full Text**: Yes\n",
            "4. **Experimental Setup or Environment**: Yes\n",
            "5. **Cannot extract concrete factors that Eased Reproducibility**: No\n",
            "------------------------------------------------------\n",
            "Reference model response (llama-3.2-1b)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Latex object>"
            ],
            "text/latex": "\\pi_{LLM-instruct}:"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I would classify this description into the following categories:\n",
            "\n",
            "1. **Availability of Code**: **Yes** (The code is publicly available on GitHub, which is a great sign of reproducibility)\n",
            "2. **Supporting Artifacts**: **Yes** (The paper provides a good overview of the data-sets and how they were compiled, which is essential for reproducing the results)\n",
            "3. **Readability of Full Text**: **Yes** (The description is clear and easy to understand, making it easy to follow the author's thought process)\n",
            "4. **Experimental Setup or Environment**: **Yes** (The description mentions the experimental setup, which is crucial for understanding the results)\n",
            "5. **Cannot extract concrete factors that Eased Reproducibility**: **No** (The description does not mention any specific challenges or limitations that would make it difficult to reproduce the results)\n",
            "------------------------------------------------------\n",
            "Correct response:\n",
            "\n",
            "<|begin_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "{\n",
            "  \"1. Availability of Code\": \"Yes\",\n",
            "  \"2. Supporting Artifacts\": \"Yes\",\n",
            "  \"3. Readability of Full Text\": \"Yes\",\n",
            "  \"4. Experimental Setup or Environment\": \"No\",\n",
            "  \"5. Cannot extract concrete factors that Eased Reproducibility\": \"No\"\n",
            "}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "        \n",
            "------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# del model, tokenizer, trainer\n",
        "# torch.cuda.empty_cache()\n",
        "# gc.collect()"
      ],
      "metadata": {
        "id": "1oU-SzFeQVAS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oPdNfbOq1bfp"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}