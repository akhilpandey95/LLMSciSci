{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zT6MyuXRlkNB"
   },
   "source": [
    "## Large Language models for Scientometrics\n",
    "\n",
    "**Large Language Models:**\n",
    "\n",
    "The capabilities of Large Language Models (**LLM's**) to process data from different modalities and excel at different tasks ranging from information extraction, question and answering, math, coding, and recently reasoning simply shows the potential of this technology. Intuitively the complexities of training these models on different datasets/data mixes, opting different architectural choices, choosing different alignment strategies **[1]** seemingly could suggest picking a specific model for each task, but **LLM's** are geared towards being considered as general task solvers.\n",
    "\n",
    "For this hands-on session we are going to use the Reproducibility dataset from the paper <u>Laying Foundations to Quantify the \"Effort of Reproducibility\"</u> **[2]** to preference tune answers using the **Direct Preference Optimization(DPO)** algorithm. *DPO* unlike other reinforcement algorithms directly applies maximum likelihood on the preference dataset to perform implicit reward modeling. Ideally, similar to most RL algorithms we would be applying the same reward maximization via **KL** divergence constraint. Theoretically, *DPO* is RL free, and doing a simple classification on a given a dataset $D$ that includes **chosen** and **rejected** responses. Learn more about *DPO* from the original paper **[3]**.\n",
    "\n",
    "**References**(s):\n",
    "<br>\n",
    "**[1]** [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)\n",
    "<br>\n",
    "**[2]** [Laying Foundations to Quantify the “Effort of Reproducibility”](https://ieeexplore.ieee.org/abstract/document/10266070)\n",
    "<br>\n",
    "**[3]** [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290)\n",
    "\n",
    "**Other Resources**:\n",
    "<br>\n",
    "**[R-1]** [Direct Preference Optimization (DPO) for LLM Alignment (From Scratch)\n",
    "](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb)\n",
    "<br>\n",
    "**[R-2]** [Preference Tuning for Summarization using Synthetic Data\n",
    "](https://github.com/anyscale/templates/tree/1939a34a54a0efeb1e86917d1175d92b50f482e6/templates/fine-tune-llm_v2/end-to-end-examples/fine-tune-preference#step-2-fine-tuning)\n",
    "\n",
    "<img src=\"https://images.ctfassets.net/cnu0m8re1exe/sIyPeDxgpIluQqQWK8nhS/67004d28ebbce2ca1f654a7a0afd92b3/SciSci.png\" align=\"center\" width=400 height=500>\n",
    "\n",
    ">(Credit: Davide Bonazzi) from [*Discover Magazine*](https://www.discovermagazine.com/the-sciences/the-science-of-science)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zgo0dfZ4qFZV"
   },
   "source": [
    "**Table of Contents**:\n",
    "- Setup\n",
    "- Prepare Preference Dataset for **Direct Preference Optimization(DPO)**\n",
    "- API & Local Models setup\n",
    "- Preference tuning via **Direct Preference Optimization(DPO)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3ymvPHaqC1G"
   },
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3HcRP_zlTbd",
    "outputId": "769f6d39-28e6-4eb3-8d38-3873b37f5e3c"
   },
   "outputs": [],
   "source": [
    "# @title 1.1 Install necessary libraries\n",
    "\n",
    "# install outlines\n",
    "print(f\"Installing latest transformers...\")\n",
    "!pip install -q git+https://github.com/huggingface/transformers\n",
    "\n",
    "# install tiktoken\n",
    "print(f\"Installing tiktoken...\")\n",
    "!pip install -q tiktoken\n",
    "\n",
    "# install outlines\n",
    "print(f\"Installing outlines...\")\n",
    "!pip install -q outlines\n",
    "\n",
    "# install huggingface-trl\n",
    "print(f\"Installing huggingface-trl...\")\n",
    "!pip install -q trl\n",
    "\n",
    "# install flash attention\n",
    "print(f\"Installing flash-attention-2...\")\n",
    "!pip install -q flash-attn --no-build-isolation\n",
    "\n",
    "# install bitsandbytes\n",
    "print(f\"Installing bitsandbytes...\")\n",
    "!pip install -q -U bitsandbytes\n",
    "\n",
    "# install openai\n",
    "print(f\"Installing openai...\")\n",
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "m3YRd_l4qQut"
   },
   "outputs": [],
   "source": [
    "# @title 1.2 Import necessary libraries\n",
    "# This Source Code Form is subject to the terms of the MIT\n",
    "# License. If a copy of the same was not distributed with this\n",
    "# file, You can obtain one at\n",
    "# https://github.com/Northwestern-CSSI/LLMSciSci/blob/main/LICENSE.\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import bs4\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import urllib3\n",
    "import pathlib\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import openai as oai\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "from peft import LoraConfig\n",
    "from ast import literal_eval\n",
    "from pydantic import BaseModel\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from collections import defaultdict\n",
    "from outlines import models, generate\n",
    "from typing import List, Optional, Union\n",
    "from datasets import Dataset, DatasetDict\n",
    "from collections import Counter, OrderedDict\n",
    "from transformers import BitsAndBytesConfig, set_seed\n",
    "from pydantic import BaseModel, create_model, RootModel\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Gemma3ForCausalLM,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    set_seed\n",
    ")\n",
    "from trl import (\n",
    "    DPOConfig,\n",
    "    DPOTrainer,\n",
    "    ModelConfig,\n",
    "    ScriptArguments,\n",
    "    TrlParser,\n",
    "    get_kbit_device_map,\n",
    "    get_peft_config,\n",
    "    get_quantization_config,\n",
    ")\n",
    "from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lYjeUijtWsl"
   },
   "source": [
    "<img src=\"https://github.com/akhilpandey95/LLMSciSci/blob/main/media/LLMSciSci_dataset.png?raw=true\" width=650 height=475>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4HoEqRGtaUe",
    "outputId": "72031895-bb99-48a9-a09d-64e5d0f45c8b"
   },
   "outputs": [],
   "source": [
    "# @title 1.3 Load `ReScience` dataset - [Download Data](https://drive.google.com/drive/folders/1qLCC5ZiDWoRtMQyBTeMxPrPlJLxcVgMN?usp=sharing)\n",
    "!ls -lah ./drive/MyDrive/CSSI/Lecture\n",
    "\n",
    "# set the directory\n",
    "os.chdir(\"./drive/MyDrive/CSSI/Lecture\")\n",
    "\n",
    "# read rescience\n",
    "rescience = pl.read_csv(\"./data/ReScience_JCDL-23.csv\")\n",
    "\n",
    "# show shape and columns\n",
    "print(\"-------------------------------\")\n",
    "print(f\"Data shape: {rescience.shape}\")\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "\"\"\"\n",
    "Data columns: ['author', 'title', 'doi', 'article_type', 'lang', 'pdf_url', 'keywords', 'review_url',\n",
    "              'code_url', 'volume', 'issue', 'year', 'abstract', 'easy', 'difficult', 'gs_citations',\n",
    "              'gs_scholar_url', 'original_pdf_url', 'original_article_url', 'reason_for_easiness',\n",
    "              'reason_for_difficulty', 'limitations_results', 'scope_of_reproducibility',\n",
    "              'original_abstract', 'orig_art_sciparse_full_text', 'orig_art_pdfminer_full_text',\n",
    "              'original_sections', 'no_hyp', 'no_alg', 'no_images', 'no_equations', 'no_tables',\n",
    "              'is_meth_pres', 'is_intro_pres', 'link_to_code_available', 'mean_readability',\n",
    "              'hyp_available_in_text', 'easiness_longform', 'difficult_longform',\n",
    "              'list_for_limitations', 'list_for_diff', 'list_for_easiness', 'more_than_one_easy']\n",
    "\"\"\"\n",
    "\n",
    "# metadata\n",
    "meta_data_columns = [\"doi\", \"title\", \"review_url\", \"easy\", \"difficult\",\n",
    "                     \"scope_of_reproducibility\", \"reason_for_easiness\", \"reason_for_difficulty\"]\n",
    "print(f\"Rescience Metadata columns of interest: {meta_data_columns}\")\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "# sneak peak of the data\n",
    "print(rescience.select(meta_data_columns).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36-emY-kZeEJ"
   },
   "source": [
    "### 2. Prepare Preference Dataset for **Direct Preference Optimization(DPO)**\n",
    "\n",
    "<img src=\"https://github.com/akhilpandey95/LLMSciSci/blob/main/media/LLMSciSci_DPO_dataset.png?raw=true\" width=700 height=450>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUx1SwX5ppwn",
    "outputId": "32c8ce79-9697-4e4c-8ad4-0b685933be2d"
   },
   "outputs": [],
   "source": [
    "# @title 2.1 Load raw preference data from `GPT`, `Gemini` and `Llama` responses\n",
    "# read the gemini labelling data\n",
    "gemini_effortly = pl.read_csv(\"./data/gemini_effortly_labels_gamma.csv\")\n",
    "\n",
    "# read the gpt labelling data\n",
    "gpt_effortly = pl.read_csv(\"./data/gpt4_effortly_labels_beta.csv\")\n",
    "\n",
    "# read the llama labelling data\n",
    "llama_effortly = pl.read_csv(\"./data/llama3_effortly_labels_beta.csv\")\n",
    "\n",
    "# show a preview of the response(s)\n",
    "print(\"---------------------------\")\n",
    "print(f\"Response from gemini\")\n",
    "print(gemini_effortly.select(\"easy_gemini_response\")[0].item())\n",
    "print(\"---------------------------\")\n",
    "print(f\"Response from gpt\")\n",
    "print(gpt_effortly.select(\"easy_gpt_response\")[0].item())\n",
    "print(\"---------------------------\")\n",
    "print(f\"Response from llama\")\n",
    "print(llama_effortly.select(\"easy_llama3_response\")[0].item())\n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "s6jdqCqzZieL"
   },
   "outputs": [],
   "source": [
    "# @title 2.2 helper class to build the $D_{ReproEffortDataset}$ preference dataset\n",
    "\n",
    "# helper function to load/initalize the prompt\n",
    "def process_prompt(raw_text, tokenizer, device, task=\"easy\", prompt_type=\"prompt\"):\n",
    "    \"\"\"\n",
    "    Given raw input text generate a prompt that will\n",
    "    be supplied to a preference dataset loader.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    arg1 | raw_text: str\n",
    "        Raw input text without prompt template\n",
    "    arg2 | tokenizer: transformers.tokenization_utils_fast.PreTrainedTokenizerFast\n",
    "        Tokenizer from the model\n",
    "    arg3 | device: str\n",
    "        Device name for the inputs and attention masks to sit on\n",
    "    arg4 | task: str[OPTIONAL]\n",
    "        Task type \"What was easy ?\" or \"What was difficult ?\"\n",
    "    arg5 | prompt_type: str[OPTIONAL]\n",
    "        String flag to be applied at the top of messages to create \"prompt\"\n",
    "        \"chosen\" or \"rejected\" chat responses for the preference dataset\n",
    "\n",
    "    Returns\n",
    "    ------------\n",
    "        Text\n",
    "    \"\"\"\n",
    "    # init\n",
    "    prompt = None\n",
    "    messages = []\n",
    "    add_generation_prompt = True\n",
    "    sys_prompt, user_prompt, input_text = None, None, None\n",
    "\n",
    "    # init system prompt available\n",
    "    sys_prompt = \"\"\"\n",
    "    You are a research assistant working on understanding the\n",
    "    spectrum of outputs researchers outline when reproducing\n",
    "    academic articles.\n",
    "    \"\"\"\n",
    "\n",
    "    # what was easy ?\n",
    "    if task == \"easy\":\n",
    "      # init user prompt for the task\n",
    "      user_prompt=\"\"\"\n",
    "      **Task:** You are given brief descriptions that made it easy for researcher\n",
    "      to reproduce original articles. Your goal is to analyze the brief description\n",
    "      and classify them into one or more from the following five categories,\n",
    "      which include:\n",
    "\n",
    "      1. Availability of Code\n",
    "      2. Supporting Artifacts\n",
    "      3. Readability of Full Text\n",
    "      4. Experimental Setup or Environment\n",
    "      5. Cannot extract concrete factors that Eased Reproducibility.\n",
    "      \"\"\"\n",
    "\n",
    "      # init the prompt\n",
    "      input_text = \"\"\"\n",
    "      **What was easy:**\n",
    "      ```plaintext\n",
    "      EASY_DESCRIPTION\n",
    "      ```\n",
    "      \"\"\"\n",
    "    else:\n",
    "      # init user prompt for the task\n",
    "      user_prompt=\"\"\"\n",
    "      **Task:** You are given brief descriptions that made it difficult for\n",
    "      researcher to reproduce original articles. Your goal is to analyze the\n",
    "      description and classify them into one or more from the following\n",
    "      five categories:\n",
    "\n",
    "      1. Missing Algorithm step or Architecture details\n",
    "      2. Missing nuance details\n",
    "      3. Unclear notation or documentation in the codebase\n",
    "      4. Insufficient Math/Equations\n",
    "      5. Cannot extract concrete factors that made it difficult for reproducibility.\n",
    "      \"\"\"\n",
    "\n",
    "      # init the prompt\n",
    "      input_text = \"\"\"\n",
    "      **What was difficult:**\n",
    "      ```\n",
    "      DIFF_DESCRIPTION\n",
    "      ```\n",
    "      \"\"\"\n",
    "\n",
    "    # apply chat template on the chosen/rejected response\n",
    "    if prompt_type == \"chosen\":\n",
    "      # set the chosen response for the preferences\n",
    "      messages.append([{\"role\": \"assistant\", \"content\": raw_text}])\n",
    "\n",
    "      # apply prompt template\n",
    "      add_generation_prompt=False\n",
    "\n",
    "      # apply prompt and remove the system prompt\n",
    "      prompt = tokenizer.apply_chat_template(messages, \\\n",
    "                                            tokenize=False, \\\n",
    "                                            use_system_prompt=add_generation_prompt, \\\n",
    "                                            add_generation_prompt=add_generation_prompt)\n",
    "    elif prompt_type == \"rejected\":\n",
    "      # set the rejected response for the preferences\n",
    "      messages.append([{\"role\": \"assistant\", \"content\": raw_text}])\n",
    "\n",
    "      # apply prompt template\n",
    "      add_generation_prompt=False\n",
    "\n",
    "      # apply prompt and remove the system prompt\n",
    "      prompt = tokenizer.apply_chat_template(messages, \\\n",
    "                                            tokenize=False, \\\n",
    "                                            use_system_prompt=add_generation_prompt, \\\n",
    "                                            add_generation_prompt=add_generation_prompt)\n",
    "    else:\n",
    "      # adjust and replace EASY_DESCRIPTION or DIFF_DESCRIPTION based on task\n",
    "      if task == \"easy\":\n",
    "        input_text = input_text.replace(\"EASY_DESCRIPTION\", raw_text)\n",
    "      else:\n",
    "        input_text = input_text.replace(\"DIFF_DESCRIPTION\", raw_text)\n",
    "\n",
    "      # set the prompt for the preferences\n",
    "      messages.append([\n",
    "          {\"role\": \"system\", \"content\": sys_prompt},\n",
    "          {\"role\": \"user\", \"content\": user_prompt + input_text}\n",
    "      ])\n",
    "\n",
    "      # apply prompt template\n",
    "      prompt = tokenizer.apply_chat_template(messages, \\\n",
    "                                            tokenize=False, \\\n",
    "                                            use_system_prompt=add_generation_prompt, \\\n",
    "                                            add_generation_prompt=add_generation_prompt)\n",
    "\n",
    "    # return the processed prompt\n",
    "    return prompt\n",
    "\n",
    "# utility class to create the preference dataset\n",
    "class ReproEffortPrefDataset:\n",
    "    def __init__(self, raw_data, tokenizer, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Given raw text prepare preference dataset.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        arg1 | raw_data: polars.DataFrame or pandas.DataFrame or List[dict]\n",
    "            ML reproducibility challenge data processed with the following columns:\n",
    "              - \"easy\": Raw prompt text for the easy task.\n",
    "              - \"easy_gpt_response\": Chosen response for the easy task.\n",
    "              - \"easy_llama3_response\": Rejected response for the easy task.\n",
    "              - \"difficult\": Raw prompt text for the difficult task.\n",
    "              - \"difficult_gpt_response\": Chosen response for the difficult task.\n",
    "              - \"diff_llama3_response\": Rejected response for the difficult task.\n",
    "        arg2 | tokenizer: transformers.tokenization_utils_fast.PreTrainedTokenizerFast\n",
    "            Tokenizer from the model to apply chat template.\n",
    "        arg3 | device: str[OPTIONAL]\n",
    "            Device name (e.g., \"cpu\" or \"cuda\") for processing.\n",
    "\n",
    "        Returns\n",
    "        ------------\n",
    "            Text\n",
    "        \"\"\"\n",
    "        # polars df ? convert it to a pandas DataFrame.\n",
    "        if hasattr(raw_data, \"to_pandas\"):\n",
    "            raw_data = raw_data.to_pandas()\n",
    "        # pd df ? convert to list of dicts\n",
    "        if isinstance(raw_data, pd.DataFrame):\n",
    "            self.raw_data = raw_data.to_dict(\"records\")\n",
    "        elif isinstance(raw_data, list):\n",
    "            self.raw_data = raw_data\n",
    "        else:\n",
    "            raise ValueError(\"ERR[ReproEffortPrefDataset]: Unsupported raw_data type; must be a pl.DataFrame, pd.DataFrame, or list[dict].\")\n",
    "\n",
    "        # init default arguments\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    # helper function to build the dataset object\n",
    "    def build_dataset(self, test_size=0.2, seed=2025):\n",
    "        \"\"\"\n",
    "        Build a unified preference dataset with\n",
    "        \"What was easy ?\" and \"What was difficult ?\" texts.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        arg1 | test_size: float[OPTIONAL]\n",
    "            Set the size of the test set, defaults to 0.2 or 20% of the dataset.\n",
    "        arg2 | seed: int[OPTIONAL]\n",
    "            Seed parameter for reproducibility, defaults to 2025.\n",
    "\n",
    "        Returns\n",
    "        ------------\n",
    "            Dictionary {\"prompt\": str, \"chosen\": str, \"rejected\": str}\n",
    "        \"\"\"\n",
    "        # inti list to store results\n",
    "        records = []\n",
    "\n",
    "        # set the keys for processing\n",
    "        task_types = [\"easy\", \"difficult\"]\n",
    "        chosen_types = [\"easy_gpt_response\", \"diff_gpt_response\"]\n",
    "        # rejected_types = [\"easy_llama3_response\", \"diff_llama3_response\"]\n",
    "        rejected_types = [\"easy_gemini_response\", \"diff_gemini_response\"]\n",
    "\n",
    "        # iterate and combine \"easy\" and \"difficult\" tasks\n",
    "        for sample in self.raw_data:\n",
    "            # procdess for each task\n",
    "            for idx, task in enumerate(task_types):\n",
    "                # set the prompt\n",
    "                prompt = process_prompt(sample[task], self.tokenizer, self.device, task=task, prompt_type=\"prompt\")[0]\n",
    "\n",
    "                # set the choosen response\n",
    "                chosen = process_prompt(sample[chosen_types[idx]], self.tokenizer, self.device, task=task, prompt_type=\"chosen\")[0]\n",
    "\n",
    "                # set the rejected response\n",
    "                rejected = process_prompt(sample[rejected_types[idx]], self.tokenizer, self.device, task=task, prompt_type=\"rejected\")[0]\n",
    "\n",
    "                # append the records\n",
    "                records.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"chosen\": chosen,\n",
    "                    \"rejected\": rejected\n",
    "                })\n",
    "\n",
    "        # merge records\n",
    "        combined_data = {\n",
    "            \"prompt\": [r[\"prompt\"] for r in records],\n",
    "            \"chosen\": [r[\"chosen\"] for r in records],\n",
    "            \"rejected\": [r[\"rejected\"] for r in records],\n",
    "        }\n",
    "\n",
    "        # init hf dataset and perform train/test split.\n",
    "        dataset = Dataset.from_dict(combined_data)\n",
    "\n",
    "        # shuffle the datset\n",
    "        dataset = dataset.shuffle()\n",
    "\n",
    "        # train test split\n",
    "        dataset_split = dataset.train_test_split(test_size=test_size, seed=seed)\n",
    "\n",
    "        # return final dataset\n",
    "        return dataset_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5rVre_oA4MD"
   },
   "source": [
    "### 3. API & Local Models setup\n",
    "\n",
    "For the commercial models you would need to setup your account and obtainan API key to run some of the experiments in this notebook.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Pre-requisites for commercial models**\n",
    "<br>\n",
    "**OpenAI**: https://platform.openai.com/settings/organization/api-keys\n",
    "<hr>\n",
    "\n",
    "**Pre-requisites for local models**\n",
    "<br>\n",
    "The experiments and widgets in the notebook require `data/` and `models/`. Since `data/` is loaded, we need model weights which can be downloaded here:\n",
    "- [Models](https://drive.google.com/drive/folders/1aNT1SNA7Lz9kMgt5p1yGWST1T6D2Dmbd?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4mPh5ueBANa",
    "outputId": "452b5778-5134-46df-a874-c6fecea4fecb"
   },
   "outputs": [],
   "source": [
    "# @title 3.1 Local model Catalog\n",
    "!ls -lah models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pwQaMpcXBDrh"
   },
   "outputs": [],
   "source": [
    "# @title 3.2 Load model client or model-tokenizer pair\n",
    "# helper function to load/initalize the model\n",
    "def load_model(model_name, device):\n",
    "    \"\"\"\n",
    "    Given a model path, load tokenizer-model\n",
    "    pair and return the objects tagged to the\n",
    "    given device (cpu/cuda)\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    arg1 | model_name: str\n",
    "        Use model catalog to load local model weights\n",
    "    arg2 | device: str\n",
    "        Hardware acceleration, defaults to \"cpu\" if any errors arise\n",
    "\n",
    "    Returns\n",
    "    ------------\n",
    "        Tuple(AutoModel, AutoTokenizer) for local (model_client, model_name)\n",
    "    \"\"\"\n",
    "    # device for acceleration\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif torch.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    # local models\n",
    "    local_models = [\"llama3.2-1b\", \"llama3.2-3b\", \"llama3.1-8b\", \"qwen2.5-1.5b\", \"r1-distill-qwen-1.5b\"]\n",
    "\n",
    "    # pathlib for models\n",
    "    model_path = pathlib.Path(\"/content/drive/MyDrive/CSSI/Lecture\")\n",
    "\n",
    "    # set the model-id\n",
    "    model_catalog = {\n",
    "        \"llama-3.2-1b\": model_path/f\"models/Llama3.2-1B-Instruct/\",\n",
    "        \"llama-3.2-3b\": model_path/f\"models/Llama3.2-3B-Instruct/hf/\",\n",
    "        \"llama-3.1-8b\": model_path/f\"models/Meta-Llama-3.1-8B-Instruct/hf/\",\n",
    "        \"gemma-3-4b\": model_path/f\"models/gemma-3-4b-it/\",\n",
    "        \"qwen-2.5-1.5b\": model_path/f\"models/Qwen2.5-1.5B-Instruct/\"\n",
    "    }\n",
    "\n",
    "    # set a model-id\n",
    "    model_id = model_catalog[model_name]\n",
    "\n",
    "    # log\n",
    "    print(\"----------------------------------\")\n",
    "    print(f\"Using {device} to load {model_id}\")\n",
    "    print(\"----------------------------------\")\n",
    "\n",
    "    # get model-tokenizer pair\n",
    "    start = time.time()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "    # based on model size switch quantization config\n",
    "    if model_name == \"llama3.1-70b\" or model_name == \"r1-distill-llama-70b\":\n",
    "        # 4-bit quantization config\n",
    "        bnb_4bit = BitsAndBytesConfig(\n",
    "          load_in_4bit=True,\n",
    "          bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "          bnb_4bit_quant_storage=torch.bfloat16\n",
    "        )\n",
    "\n",
    "        # 4 bit quantization\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, \\\n",
    "                                                   quantization_config=bnb_4bit, \\\n",
    "                                                   trust_remote_code=True, \\\n",
    "                                                   low_cpu_mem_usage=True, \\\n",
    "                                                   attn_implementation=\"sdpa\", \\\n",
    "                                                   device_map=device)\n",
    "    elif model_name == \"gemma-3-4b\":\n",
    "        model = Gemma3ForCausalLM.from_pretrained(model_id, \\\n",
    "                                              trust_remote_code=True, \\\n",
    "                                              torch_dtype=torch.bfloat16, \\\n",
    "                                              low_cpu_mem_usage=True, \\\n",
    "                                              attn_implementation=\"sdpa\", \\\n",
    "                                              device_map=device)\n",
    "    else:\n",
    "      # 4-bit quantization config\n",
    "      bnb_4bit = BitsAndBytesConfig(\n",
    "          load_in_4bit=True,\n",
    "          bnb_4bit_use_double_quant=True,\n",
    "          bnb_4bit_quant_type=\"nf4\",\n",
    "          bnb_4bit_compute_dtype=torch.bfloat16\n",
    "      )\n",
    "\n",
    "      # load bfloat16 weights\n",
    "      model = AutoModelForCausalLM.from_pretrained(model_id, \\\n",
    "                                                   trust_remote_code=True, \\\n",
    "                                                   torch_dtype=torch.bfloat16, \\\n",
    "                                                   low_cpu_mem_usage=True, \\\n",
    "                                                   attn_implementation=\"flash_attention_2\", \\\n",
    "                                                   device_map=device)\n",
    "\n",
    "    # is it a llama tokenizer ?\n",
    "    if \"llama\" in model_name:\n",
    "        # pad token if needed\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"<|finetune_right_pad_id|>\"})\n",
    "        print(f\"Setting <|finetune_right_pad_id|> token for {model_id}\")\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        # llama prompt template\n",
    "        llama_template = r\"\"\"\n",
    "        {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
    "        \"\"\"\n",
    "\n",
    "        # set the chat template\n",
    "        tokenizer.chat_template = llama_template\n",
    "\n",
    "    # gemma eos token\n",
    "    if \"gemma\" in model_name:\n",
    "        # set the EOS tokenid\n",
    "        tokenizer.eos_token_id = tokenizer.encode(\"<end_of_turn>\")[0]\n",
    "\n",
    "    # load time\n",
    "    end = time.time()\n",
    "    print(f\"Model-tokenizer Load Time:, {end - start} seconds\")\n",
    "    print(\"----------------------------------\")\n",
    "\n",
    "    # return the pair\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H1WIFUjnTg-k",
    "outputId": "5df5b196-9ef7-4650-d0d6-24a7ab8123a5"
   },
   "outputs": [],
   "source": [
    "# @title 3.3 Load policy model for $\\pi_{LLMSciSci}$\n",
    "# load the model for policy update\n",
    "# model, tokenizer = load_model(\"qwen-2.5-1.5b\", \"cuda\")\n",
    "model, tokenizer = load_model(\"llama-3.2-1b\", \"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy-zyRUbjg46"
   },
   "source": [
    "### 4. Preference tuning via **Direct Preference Optimization(DPO)**\n",
    "\n",
    "$$\n",
    "L_{DPO}(\\pi_{LLMSciSci}: \\pi_{LLM-instruct})\n",
    "\\;=\\; - \\,\\mathbb{E}{\\bigl(x,\\,r^+,\\,r^-\\bigr) \\sim D_{ReproEffortDataset}}\n",
    "\\Bigl[\n",
    "\\log \\,\\sigma\\!\\Bigl(\n",
    "r_\\theta(x,r^+) \\;-\\; r_\\theta(x,r^-)\n",
    "\\Bigr)\n",
    "\\Bigr]\n",
    "$$\n",
    "\n",
    "$$\n",
    "r_\\theta(x, r)\n",
    "\\;=\\;\n",
    "\\beta \\,\\log \\frac{\\pi_{LLMSciSci}(r \\,\\vert\\, x)}{\\pi_{LLM-instruct}(r \\,\\vert\\, x)}\n",
    "$$\n",
    "\n",
    "where the $r_{\\theta}$ is computed\n",
    "- using $r^+$(human preferred response), and $r^-$(rejected responses).\n",
    "- for the models $\\pi_{LLMSciSci}$ and $\\pi_{LLM-instruct}$.\n",
    "- $r_{\\theta}$  captures the log-probability of the *chosen* vs *rejected* responses on $D_{ReproEffortDataset}$.\n",
    "- $\\pi_{LLM-instruct}$ is the instruct-tuned open weight reference model.\n",
    "- $\\pi_{LLMSciSci}$ is the final RL model intended to be preference-tuned on $D_{ReproEffortDataset}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PWOkOxBpM24",
    "outputId": "a3d533bc-5678-4cc1-fc1c-c02d07185c5b"
   },
   "outputs": [],
   "source": [
    "# @title 4.1 Init `ReproEffortPrefDataset` dataset object, train/test split\n",
    "\n",
    "# # combine the GPT and Llama datasets\n",
    "# raw_data = gpt_effortly.join(\n",
    "#     llama_effortly.select([\"doi\", \"easy\", \"difficult\", \"easy_llama3_response\", \"easy_llama3_label\", \"diff_llama3_response\", \"diff_llama3_label\"]),\n",
    "#     on=\"doi\",\n",
    "#     how=\"inner\"\n",
    "# )\n",
    "\n",
    "# combine the GPT and Gemini datasets\n",
    "raw_data = gpt_effortly.join(\n",
    "    gemini_effortly.select([\"doi\", \"easy\", \"difficult\", \"easy_gemini_response\", \"easy_gemini_label\", \"diff_gemini_response\", \"diff_gemini_label\"]),\n",
    "    on=\"doi\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# final shape of the raw data\n",
    "print(f\"-----------------------------------\")\n",
    "print(f\"Shape of raw_data: {raw_data.shape}\")\n",
    "print(f\"-----------------------------------\")\n",
    "print(\"Columns in raw_data:\")\n",
    "print(f\"-----------------------------------\")\n",
    "pprint(raw_data.columns)\n",
    "print(f\"-----------------------------------\")\n",
    "\n",
    "# convert raw data to pandas\n",
    "raw_data_pd = raw_data.to_pandas()\n",
    "\n",
    "# create the preference dataset\n",
    "dataset_obj = ReproEffortPrefDataset(raw_data_pd, tokenizer, device=\"cuda\")\n",
    "dataset = dataset_obj.build_dataset(test_size=0.2, seed=2025)\n",
    "print(f\"-----------------------------------\")\n",
    "print(f\"ReproEffortPrefDataset:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qd5DGcihsh_H",
    "outputId": "93b5a73a-2279-4a57-f0d5-4de169964895"
   },
   "outputs": [],
   "source": [
    "# @title 4.2 Preview of the preference dataset for \"What was easy ?\" and \"What was difficult ?\" tasks\n",
    "\n",
    "# print sample cut of the dataset\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Prompt for the preference dataset..\")\n",
    "print(dataset[\"train\"][0][\"prompt\"])\n",
    "print(\"-----------------------------------\")\n",
    "print(\"CHOSEN response:\")\n",
    "print(dataset[\"train\"][0][\"chosen\"])\n",
    "print(\"-----------------------------------\")\n",
    "print(\"REJECTED response:\")\n",
    "print(dataset[\"train\"][0][\"rejected\"])\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 150,
     "referenced_widgets": [
      "4cc393b69f0f41e0924fe35a411d0edd",
      "503b6506f8ba453a85752167819adaa0",
      "c5465260b6334755b24a8017e4f539ce",
      "54de078cf1e742c3846e52afe298c5df",
      "bbe14e55767a477e99dacc35a4abef7f",
      "353cd29237b04c5795dddf21a66f643d",
      "505deeb266204eec8dab2d532bc212bf",
      "734a622240cf430db963a29be00d0b65",
      "cb9187ea5b3841bda4db8aaa7242c0a4",
      "a74447ac854f4fc397d6796b6affaa06",
      "cf44985272b649dc94226638f167cf87",
      "7ac5db6db076420ba73a2477ea09f443",
      "9c26e7ca2bc9473889954a4c7e3d5e7e",
      "4911111db1ff46a5ba8ea6079b4e52e2",
      "a02456a101db46d99fe0bc0fc4fc0322",
      "d181bd1a5ba34c81b5b7c37efa384d8f",
      "e9d72d210ded4735b06b84bebe1f025e",
      "bed96901152a4bdf94bda3f2a723135e",
      "bddc69906dc740b4995fe76c5f4e43d0",
      "fbe7809140e54179bc1d47b98d5271c2",
      "2ad113ed420a4e798cde017944e5798c",
      "b60d135df6be477dad0309d59bd04338"
     ]
    },
    "id": "DWEUbK_x16Bn",
    "outputId": "c8af4538-a575-43ab-8bba-81fcc2089ba8"
   },
   "outputs": [],
   "source": [
    "# @title 4.3 Tokenomics to decide `max_seq_length` and `prompt_length`\n",
    "# gather the train and test datasets\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# lets find the p95 length of the prompt\n",
    "prompt_length = int(np.percentile([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset[\"prompt\"]], 95))\n",
    "max_seq_length_chosen = int(np.percentile([len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) for x in train_dataset], 95))\n",
    "max_seq_length_rejected = int(np.percentile([len(tokenizer(x[\"prompt\"] + x[\"rejected\"])[\"input_ids\"]) for x in train_dataset], 95))\n",
    "max_seq_length = max(max_seq_length_chosen, max_seq_length_rejected)\n",
    "\n",
    "# filter datasets to remove samples that are too long\n",
    "train_dataset = train_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) <= max_seq_length)\n",
    "test_dataset = test_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) <= max_seq_length)\n",
    "print(f\"len(train_dataset): {len(train_dataset)}\")\n",
    "print(f\"len(test_dataset): {len(test_dataset)}\")\n",
    "\n",
    "# Up the lengths to next multiple of 2, why 2? Don't know\n",
    "prompt_length = ((prompt_length + 1) // 2) * 2\n",
    "max_seq_length = ((max_seq_length + 1) // 2) * 2\n",
    "print(f\"p95 prompt length: {prompt_length}\")\n",
    "print(f\"p95 prompt + chosen length: {max_seq_length}\")\n",
    "\n",
    "# prompt_length = 512\n",
    "# max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7EOED_aVNzus"
   },
   "outputs": [],
   "source": [
    "# @title 4.4 Train the first $\\pi_{LLMSciSci}$ policy via $L_{DPO}$ using the $\\sigma$ loss\n",
    "\n",
    "# # LoRA config\n",
    "# peft_config = LoraConfig(\n",
    "#     lora_alpha=128,\n",
    "#     lora_dropout=0.05,\n",
    "#     r=256,\n",
    "#     bias=\"none\",\n",
    "#     target_modules=\"all-linear\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "# # dpo params\n",
    "# dpo_args = {\n",
    "#     \"beta\": 0.3,\n",
    "#     \"loss_type\": \"sigmoid\"\n",
    "# }\n",
    "\n",
    "# # args\n",
    "# training_args = DPOConfig(output_dir=\"llmscisci-DPO-sigmoid-beta-0.3\", \\\n",
    "#                           run_name=\"rn-llmscisci-DPO-sigmoid-beta-0.3\", \\\n",
    "#                           logging_steps=10, \\\n",
    "#                           num_train_epochs=10, \\\n",
    "#                           max_length=max_seq_length, \\\n",
    "#                           max_prompt_length=prompt_length, \\\n",
    "#                           beta=dpo_args[\"beta\"], \\\n",
    "#                           loss_type=dpo_args[\"loss_type\"], \\\n",
    "#                           label_names=[\"chosen\", \"rejected\"])\n",
    "\n",
    "# # init DPO trainer\n",
    "# trainer = DPOTrainer(model=model, \\\n",
    "#                      peft_config=peft_config, \\\n",
    "#                      args=training_args, \\\n",
    "#                      processing_class=tokenizer, \\\n",
    "#                      train_dataset=train_dataset)\n",
    "\n",
    "# # train\n",
    "# trainer.train()\n",
    "\n",
    "# # save model weights\n",
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Q1eUTMGGXLgq"
   },
   "outputs": [],
   "source": [
    "# @title 4.5 Train the first $\\pi_{LLMSciSci}$ policy via $L_{DPO}$ using the $WPO$ loss\n",
    "\n",
    "# # LoRA config\n",
    "# peft_config = LoraConfig(\n",
    "#     lora_alpha=128,\n",
    "#     lora_dropout=0.05,\n",
    "#     r=256,\n",
    "#     bias=\"none\",\n",
    "#     target_modules=\"all-linear\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "# # dpo params\n",
    "# dpo_args = {\n",
    "#     \"beta\": 0.1,                            # The beta factor in DPO loss. Higher beta means less divergence\n",
    "#     \"loss_type\": \"sigmoid\"                  # The loss type for DPO.\n",
    "# }\n",
    "\n",
    "# # args\n",
    "# training_args = DPOConfig(output_dir=\"llmscisci-DPO-WPO-gamma\", \\\n",
    "#                           run_name=\"rn-llmscisci-DPO-WPO-gamma\", \\\n",
    "#                           use_weighting=True, \\\n",
    "#                           logging_steps=10, \\\n",
    "#                           num_train_epochs=10, \\\n",
    "#                           max_length=max_seq_length, \\\n",
    "#                           max_prompt_length=prompt_length, \\\n",
    "#                           beta=dpo_args[\"beta\"], \\\n",
    "#                           loss_type=dpo_args[\"loss_type\"], \\\n",
    "#                           label_names=[\"chosen\", \"rejected\"])\n",
    "\n",
    "# # init DPO trainer\n",
    "# trainer = DPOTrainer(model=model, \\\n",
    "#                      peft_config=peft_config, \\\n",
    "#                      args=training_args, \\\n",
    "#                      processing_class=tokenizer, \\\n",
    "#                      train_dataset=train_dataset)\n",
    "\n",
    "# # train\n",
    "# trainer.train()\n",
    "\n",
    "# # save model weights\n",
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ZKTiekjuboc_"
   },
   "outputs": [],
   "source": [
    "# @title 4.6 Train the first $\\pi_{LLMSciSci}$ policy via $L_{DPO}$ using the $rDPO$ loss with $\\epsilon = 0.2$, label_smoothening=0.05\n",
    "\n",
    "# # LoRA config\n",
    "# peft_config = LoraConfig(\n",
    "#     lora_alpha=128,\n",
    "#     lora_dropout=0.05,\n",
    "#     r=256,\n",
    "#     bias=\"none\",\n",
    "#     target_modules=\"all-linear\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "# # dpo params\n",
    "# dpo_args = {\n",
    "#     \"beta\": 0.3,\n",
    "#     \"loss_type\": \"robust\"\n",
    "# }\n",
    "\n",
    "# # args\n",
    "# training_args = DPOConfig(output_dir=\"llmscisci-DPO-rob_ep_0.3-tmp\", \\\n",
    "#                           run_name=\"rn-llmscisci-DPO-rob_ep_0.3-tmp\", \\\n",
    "#                           label_smoothing=0.05, \\\n",
    "#                           logging_steps=10, \\\n",
    "#                           num_train_epochs=10, \\\n",
    "#                           max_length=max_seq_length, \\\n",
    "#                           max_prompt_length=prompt_length, \\\n",
    "#                           beta=dpo_args[\"beta\"], \\\n",
    "#                           loss_type=dpo_args[\"loss_type\"], \\\n",
    "#                           label_names=[\"chosen\", \"rejected\"])\n",
    "\n",
    "# # init DPO trainer\n",
    "# trainer = DPOTrainer(model=model, \\\n",
    "#                      peft_config=peft_config, \\\n",
    "#                      args=training_args, \\\n",
    "#                      processing_class=tokenizer, \\\n",
    "#                      train_dataset=train_dataset)\n",
    "\n",
    "# # train\n",
    "# trainer.train()\n",
    "\n",
    "# # save model weights\n",
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "efbe6f3ced4d4718aad0d58277817d0c",
      "78e67937ea644f26a98527fc1b4daadf",
      "021cb3eea4ba4c0c88efc2c08eb013d0",
      "198f468750684724a3e04061ecace97e",
      "304488de0ca743f3a4215eb5b108d021",
      "e7665807ec064d7795cc456c7f0927f0",
      "6ad6773107c747f19849e89b848501d4",
      "b8abd5b189034104a2c3c873623c15bd",
      "6427555c29024677827ad9f1e77ad0a8",
      "f6e0794f32094553934df7c9a03f2b26",
      "7044c683d0744cd2be0d1932599c0c42",
      "d3c881d0501c4e5194301d92ceaf48a5",
      "6b66ccc5481b4fb0a2cf38c599f75902",
      "f7f76102e0574fc19040da9f4d8f4dce",
      "29c2211c24bb4432a63ba2b222b0f666",
      "d3dee70691924ea78e03dcbc98406136",
      "d3c19d59c0ff43beab596e7cc918c2eb",
      "39419fba4c244ab1b13fa06a6f2b6785",
      "71249d35fd3e4dc8bcbab5f4d4980e6d",
      "d6c4a55a92cf4ed98400adde820a01c3",
      "42992185ee814518aa0bf9421ed56441",
      "2684580110c5413d908f84fe791d0607",
      "688b9b1900e74561b4d2d36f465f75d7",
      "6414ef3fc2b444aaafbb97dbfadec3e3",
      "2222945f7eda48659491b69e4780dd8d",
      "a885d6b57bdd41aaa8a2187fe8787470",
      "6b316bcbab6a4eaa870ff47b92728dc7",
      "a08fb8fa67c149f7922aeada5837c90a",
      "b511232773a14b7b9e99c3cbb1535baa",
      "e03fcc7fe09944bc84472f9227a68689",
      "58c1e9afd19042809eece5cc9c2f26ff",
      "9ef027df766344c1b4551bfb6802d8d6",
      "e9a3c8bc6a914d229b7574fc4951786a"
     ]
    },
    "id": "Fx7YnRTzHE0o",
    "outputId": "eba3540b-9aac-425f-e035-ca743eac488f"
   },
   "outputs": [],
   "source": [
    "# @title 4.7 Train the first $\\pi_{LLMSciSci}$ policy via $L_{DPO}$ using the $hinge$ loss with $\\beta = 0.05$\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.05,\n",
    "    r=256,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# dpo params\n",
    "dpo_args = {\n",
    "    \"beta\": 0.05,\n",
    "    \"loss_type\": \"hinge\"\n",
    "}\n",
    "\n",
    "# args\n",
    "training_args = DPOConfig(output_dir=\"llmscisci-DPO-best\", \\\n",
    "                          run_name=\"rn-llmscisci-DPO-best\", \\\n",
    "                          logging_steps=10, \\\n",
    "                          num_train_epochs=10, \\\n",
    "                          max_length=max_seq_length, \\\n",
    "                          max_prompt_length=prompt_length, \\\n",
    "                          beta=dpo_args[\"beta\"], \\\n",
    "                          loss_type=dpo_args[\"loss_type\"], \\\n",
    "                          label_names=[\"chosen\", \"rejected\"])\n",
    "\n",
    "# init DPO trainer\n",
    "trainer = DPOTrainer(model=model, \\\n",
    "                     peft_config=peft_config, \\\n",
    "                     args=training_args, \\\n",
    "                     processing_class=tokenizer, \\\n",
    "                     train_dataset=train_dataset)\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "\n",
    "# save model weights\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9afKJETVwKl"
   },
   "source": [
    "### 5. Inference check $\\pi_{LLMSciSci}$ against $\\pi_{LLM-instruct}$ on sample outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwkiJSp7WFHB",
    "outputId": "9e06a217-fbda-42d9-fb01-8d6b9ba8eff1"
   },
   "outputs": [],
   "source": [
    "# @title 5.1 Load reference model for $\\pi_{LLM-Instruct}$\n",
    "# load reference model without policy update\n",
    "ref_model, _ = load_model(\"llama-3.2-1b\", \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 670
    },
    "id": "gX5dFJ3OjcIM",
    "outputId": "c9685f6c-b3bb-4549-e0ee-54b91bc5de14"
   },
   "outputs": [],
   "source": [
    "# @title 5.2 Sample inference check on $D_{test}$\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "# seed for reproducibility\n",
    "set_seed(2025)\n",
    "\n",
    "# set top_p and temperature to none\n",
    "ref_model.generation_config.temperature=None\n",
    "ref_model.generation_config.top_p=None\n",
    "trainer.model.generation_config.temperature=None\n",
    "trainer.model.generation_config.top_p=None\n",
    "\n",
    "# inputs, attention mask, and shape\n",
    "idx = 0\n",
    "input_encoded = tokenizer(test_dataset[\"prompt\"][idx], padding=True, return_tensors=\"pt\")\n",
    "input_encoded_ids = input_encoded[\"input_ids\"].to(\"cuda\")\n",
    "input_encoded_attn_mask = input_encoded[\"attention_mask\"].to(\"cuda\")\n",
    "ref_input_encoded_ids = input_encoded[\"input_ids\"].to(\"cuda\")\n",
    "ref_input_encoded_attn_mask = input_encoded[\"attention_mask\"].to(\"cuda\")\n",
    "input_shape = len(input_encoded[\"input_ids\"][0])\n",
    "\n",
    "# model outputs on test prompts\n",
    "outputs = trainer.model.generate(\n",
    "    input_ids=input_encoded_ids,\n",
    "    attention_mask=input_encoded_attn_mask,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.encode(\"<|eot_id|>\")\n",
    ")\n",
    "\n",
    "# reference model outputs on test prompts\n",
    "ref_outputs = ref_model.generate(\n",
    "    input_ids=ref_input_encoded_ids,\n",
    "    attention_mask=ref_input_encoded_attn_mask,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.encode(\"<|eot_id|>\")\n",
    ")\n",
    "\n",
    "print(\"------------------------------------------------------\")\n",
    "# print(\"Input:\")\n",
    "# print(test_dataset[\"prompt\"][idx])\n",
    "# print(\"------------------------------------------------------\")\n",
    "print(\"Policy model response\")\n",
    "display(Latex(r'\\pi_{LLMSciSci}:'))\n",
    "output = tokenizer.decode(outputs[0][input_shape:], skip_special_tokens=True)\n",
    "print(output)\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Reference model response (llama-3.2-1b)\")\n",
    "display(Latex(r'\\pi_{LLM-instruct}:'))\n",
    "ref_output = tokenizer.decode(ref_outputs[0][input_shape:], skip_special_tokens=True)\n",
    "print(ref_output)\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Correct response:\")\n",
    "print(test_dataset[\"chosen\"][idx])\n",
    "print(\"------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oU-SzFeQVAS"
   },
   "outputs": [],
   "source": [
    "# del model, tokenizer, trainer\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPdNfbOq1bfp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "021cb3eea4ba4c0c88efc2c08eb013d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b8abd5b189034104a2c3c873623c15bd",
      "max": 183,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6427555c29024677827ad9f1e77ad0a8",
      "value": 183
     }
    },
    "198f468750684724a3e04061ecace97e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6e0794f32094553934df7c9a03f2b26",
      "placeholder": "​",
      "style": "IPY_MODEL_7044c683d0744cd2be0d1932599c0c42",
      "value": " 183/183 [00:00&lt;00:00, 4314.23 examples/s]"
     }
    },
    "2222945f7eda48659491b69e4780dd8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e03fcc7fe09944bc84472f9227a68689",
      "max": 183,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_58c1e9afd19042809eece5cc9c2f26ff",
      "value": 183
     }
    },
    "2684580110c5413d908f84fe791d0607": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "29c2211c24bb4432a63ba2b222b0f666": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42992185ee814518aa0bf9421ed56441",
      "placeholder": "​",
      "style": "IPY_MODEL_2684580110c5413d908f84fe791d0607",
      "value": " 183/183 [00:00&lt;00:00, 5243.38 examples/s]"
     }
    },
    "2ad113ed420a4e798cde017944e5798c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "304488de0ca743f3a4215eb5b108d021": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "353cd29237b04c5795dddf21a66f643d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39419fba4c244ab1b13fa06a6f2b6785": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42992185ee814518aa0bf9421ed56441": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4911111db1ff46a5ba8ea6079b4e52e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bddc69906dc740b4995fe76c5f4e43d0",
      "max": 46,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fbe7809140e54179bc1d47b98d5271c2",
      "value": 46
     }
    },
    "4cc393b69f0f41e0924fe35a411d0edd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_503b6506f8ba453a85752167819adaa0",
       "IPY_MODEL_c5465260b6334755b24a8017e4f539ce",
       "IPY_MODEL_54de078cf1e742c3846e52afe298c5df"
      ],
      "layout": "IPY_MODEL_bbe14e55767a477e99dacc35a4abef7f"
     }
    },
    "503b6506f8ba453a85752167819adaa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_353cd29237b04c5795dddf21a66f643d",
      "placeholder": "​",
      "style": "IPY_MODEL_505deeb266204eec8dab2d532bc212bf",
      "value": "Filter: 100%"
     }
    },
    "505deeb266204eec8dab2d532bc212bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "54de078cf1e742c3846e52afe298c5df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a74447ac854f4fc397d6796b6affaa06",
      "placeholder": "​",
      "style": "IPY_MODEL_cf44985272b649dc94226638f167cf87",
      "value": " 184/184 [00:00&lt;00:00, 887.82 examples/s]"
     }
    },
    "58c1e9afd19042809eece5cc9c2f26ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6414ef3fc2b444aaafbb97dbfadec3e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a08fb8fa67c149f7922aeada5837c90a",
      "placeholder": "​",
      "style": "IPY_MODEL_b511232773a14b7b9e99c3cbb1535baa",
      "value": "Tokenizing train dataset: 100%"
     }
    },
    "6427555c29024677827ad9f1e77ad0a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "688b9b1900e74561b4d2d36f465f75d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6414ef3fc2b444aaafbb97dbfadec3e3",
       "IPY_MODEL_2222945f7eda48659491b69e4780dd8d",
       "IPY_MODEL_a885d6b57bdd41aaa8a2187fe8787470"
      ],
      "layout": "IPY_MODEL_6b316bcbab6a4eaa870ff47b92728dc7"
     }
    },
    "6ad6773107c747f19849e89b848501d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b316bcbab6a4eaa870ff47b92728dc7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b66ccc5481b4fb0a2cf38c599f75902": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3c19d59c0ff43beab596e7cc918c2eb",
      "placeholder": "​",
      "style": "IPY_MODEL_39419fba4c244ab1b13fa06a6f2b6785",
      "value": "Applying chat template to train dataset: 100%"
     }
    },
    "7044c683d0744cd2be0d1932599c0c42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "71249d35fd3e4dc8bcbab5f4d4980e6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "734a622240cf430db963a29be00d0b65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78e67937ea644f26a98527fc1b4daadf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7665807ec064d7795cc456c7f0927f0",
      "placeholder": "​",
      "style": "IPY_MODEL_6ad6773107c747f19849e89b848501d4",
      "value": "Extracting prompt in train dataset: 100%"
     }
    },
    "7ac5db6db076420ba73a2477ea09f443": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9c26e7ca2bc9473889954a4c7e3d5e7e",
       "IPY_MODEL_4911111db1ff46a5ba8ea6079b4e52e2",
       "IPY_MODEL_a02456a101db46d99fe0bc0fc4fc0322"
      ],
      "layout": "IPY_MODEL_d181bd1a5ba34c81b5b7c37efa384d8f"
     }
    },
    "9c26e7ca2bc9473889954a4c7e3d5e7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9d72d210ded4735b06b84bebe1f025e",
      "placeholder": "​",
      "style": "IPY_MODEL_bed96901152a4bdf94bda3f2a723135e",
      "value": "Filter: 100%"
     }
    },
    "9ef027df766344c1b4551bfb6802d8d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a02456a101db46d99fe0bc0fc4fc0322": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ad113ed420a4e798cde017944e5798c",
      "placeholder": "​",
      "style": "IPY_MODEL_b60d135df6be477dad0309d59bd04338",
      "value": " 46/46 [00:00&lt;00:00, 790.97 examples/s]"
     }
    },
    "a08fb8fa67c149f7922aeada5837c90a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a74447ac854f4fc397d6796b6affaa06": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a885d6b57bdd41aaa8a2187fe8787470": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ef027df766344c1b4551bfb6802d8d6",
      "placeholder": "​",
      "style": "IPY_MODEL_e9a3c8bc6a914d229b7574fc4951786a",
      "value": " 183/183 [00:00&lt;00:00, 459.55 examples/s]"
     }
    },
    "b511232773a14b7b9e99c3cbb1535baa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b60d135df6be477dad0309d59bd04338": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b8abd5b189034104a2c3c873623c15bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbe14e55767a477e99dacc35a4abef7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bddc69906dc740b4995fe76c5f4e43d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bed96901152a4bdf94bda3f2a723135e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c5465260b6334755b24a8017e4f539ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_734a622240cf430db963a29be00d0b65",
      "max": 184,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb9187ea5b3841bda4db8aaa7242c0a4",
      "value": 184
     }
    },
    "cb9187ea5b3841bda4db8aaa7242c0a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cf44985272b649dc94226638f167cf87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d181bd1a5ba34c81b5b7c37efa384d8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3c19d59c0ff43beab596e7cc918c2eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3c881d0501c4e5194301d92ceaf48a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6b66ccc5481b4fb0a2cf38c599f75902",
       "IPY_MODEL_f7f76102e0574fc19040da9f4d8f4dce",
       "IPY_MODEL_29c2211c24bb4432a63ba2b222b0f666"
      ],
      "layout": "IPY_MODEL_d3dee70691924ea78e03dcbc98406136"
     }
    },
    "d3dee70691924ea78e03dcbc98406136": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6c4a55a92cf4ed98400adde820a01c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e03fcc7fe09944bc84472f9227a68689": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7665807ec064d7795cc456c7f0927f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9a3c8bc6a914d229b7574fc4951786a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9d72d210ded4735b06b84bebe1f025e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efbe6f3ced4d4718aad0d58277817d0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_78e67937ea644f26a98527fc1b4daadf",
       "IPY_MODEL_021cb3eea4ba4c0c88efc2c08eb013d0",
       "IPY_MODEL_198f468750684724a3e04061ecace97e"
      ],
      "layout": "IPY_MODEL_304488de0ca743f3a4215eb5b108d021"
     }
    },
    "f6e0794f32094553934df7c9a03f2b26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7f76102e0574fc19040da9f4d8f4dce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71249d35fd3e4dc8bcbab5f4d4980e6d",
      "max": 183,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d6c4a55a92cf4ed98400adde820a01c3",
      "value": 183
     }
    },
    "fbe7809140e54179bc1d47b98d5271c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
