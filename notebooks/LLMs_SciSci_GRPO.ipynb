{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Un1QckAcUr1"
   },
   "source": [
    "## Large Language models for Scientometrics\n",
    "\n",
    "**Large Language Models:**\n",
    "\n",
    "The capabilities of Large Language Models (**LLM's**) to process data from different modalities and excel at different tasks ranging from information extraction, question and answering, math, coding, and recently reasoning simply shows the potential of this technology. Intuitively the complexities of training these models on different datasets/data mixes, opting different architectural choices, choosing different alignment strategies **[1]** seemingly could suggest picking a specific model for each task, but **LLM's** are geared towards being considered as general task solvers.\n",
    "\n",
    "For this hands-on session we are going to use the Reproducibility dataset from the paper <u>Laying Foundations to Quantify the \"Effort of Reproducibility\"</u> **[2]** to preference tune answers using the **Group Relative Policy Optimization(GRPO)** algorithm. *GRPO* **[3]** was introduced by Deepseek to tackle mathenatical reasoning\n",
    "\n",
    "**References**(s):\n",
    "<br>\n",
    "**[1]** [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)\n",
    "<br>\n",
    "**[2]** [Laying Foundations to Quantify the “Effort of Reproducibility”](https://ieeexplore.ieee.org/abstract/document/10266070)\n",
    "<br>\n",
    "**[3]** [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/pdf/2402.03300)\n",
    "\n",
    "**Other Resources**:\n",
    "\n",
    "<img src=\"https://images.ctfassets.net/cnu0m8re1exe/sIyPeDxgpIluQqQWK8nhS/67004d28ebbce2ca1f654a7a0afd92b3/SciSci.png\" align=\"center\" width=400 height=500>\n",
    "\n",
    ">(Credit: Davide Bonazzi) from [*Discover Magazine*](https://www.discovermagazine.com/the-sciences/the-science-of-science)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bK_Rc2YyghXK"
   },
   "source": [
    "**Table of Contents**:\n",
    "- Setup\n",
    "- Prepare Preference Dataset for **Group Relative Policy Optimization(GRPO)**\n",
    "- API & Local Models setup\n",
    "- Preference tuning via **Group Relative Policy Optimization(GRPO)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6YUQGvUgsAJ"
   },
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2cLee0lcPsF",
    "outputId": "4f99ff3b-cf99-4b14-8e79-5673d570b2d7"
   },
   "outputs": [],
   "source": [
    "# @title 1.1 Install necessary libraries\n",
    "\n",
    "# install outlines\n",
    "print(f\"Installing latest transformers...\")\n",
    "!pip install -q git+https://github.com/huggingface/transformers\n",
    "\n",
    "# install tiktoken\n",
    "print(f\"Installing tiktoken...\")\n",
    "!pip install -q tiktoken\n",
    "\n",
    "# install outlines\n",
    "print(f\"Installing outlines...\")\n",
    "!pip install -q outlines\n",
    "\n",
    "# install huggingface-trl\n",
    "print(f\"Installing huggingface-trl...\")\n",
    "!pip install -q trl\n",
    "\n",
    "# install flash attention\n",
    "print(f\"Installing flash-attention-2...\")\n",
    "!pip install -q flash-attn --no-build-isolation\n",
    "\n",
    "# install bitsandbytes\n",
    "print(f\"Installing bitsandbytes...\")\n",
    "!pip install -q -U bitsandbytes\n",
    "\n",
    "# install openai\n",
    "print(f\"Installing openai...\")\n",
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "PQUrezkOgylU"
   },
   "outputs": [],
   "source": [
    "# @title 1.2 Import necessary libraries\n",
    "# This Source Code Form is subject to the terms of the MIT\n",
    "# License. If a copy of the same was not distributed with this\n",
    "# file, You can obtain one at\n",
    "# https://github.com/Northwestern-CSSI/LLMSciSci/blob/main/LICENSE.\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import bs4\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import urllib3\n",
    "import pickle\n",
    "import pathlib\n",
    "import tiktoken\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import openai as oai\n",
    "import seaborn as sns\n",
    "from google import genai\n",
    "from pprint import pprint\n",
    "from peft import LoraConfig\n",
    "from ast import literal_eval\n",
    "from google.genai import types\n",
    "from tqdm.notebook import tqdm\n",
    "from pydantic import BaseModel\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from collections import defaultdict\n",
    "from outlines import models, generate\n",
    "from typing import List, Optional, Union\n",
    "from datasets import Dataset, DatasetDict\n",
    "from collections import Counter, OrderedDict\n",
    "from transformers import BitsAndBytesConfig, set_seed\n",
    "from pydantic import BaseModel, create_model, RootModel\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Gemma3ForCausalLM,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    set_seed\n",
    ")\n",
    "from trl import (\n",
    "    GRPOConfig,\n",
    "    GRPOTrainer,\n",
    "    ModelConfig,\n",
    "    ScriptArguments,\n",
    "    TrlParser,\n",
    "    get_kbit_device_map,\n",
    "    get_peft_config,\n",
    "    get_quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnZkQ2E7jfwW",
    "outputId": "722df384-c9f9-471e-e503-ea4fc5cf65e0"
   },
   "outputs": [],
   "source": [
    "# @title 1.3 Load `ReScience` dataset - [Download Data](https://drive.google.com/drive/folders/1qLCC5ZiDWoRtMQyBTeMxPrPlJLxcVgMN?usp=sharing)\n",
    "!ls -lah ./drive/MyDrive/CSSI/Lecture\n",
    "\n",
    "# set the directory\n",
    "os.chdir(\"./drive/MyDrive/CSSI/Lecture\")\n",
    "\n",
    "# read rescience\n",
    "rescience = pl.read_csv(\"./data/ReScience_JCDL-23.csv\")\n",
    "\n",
    "# show shape and columns\n",
    "print(\"-------------------------------\")\n",
    "print(f\"Data shape: {rescience.shape}\")\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "\"\"\"\n",
    "Data columns: ['author', 'title', 'doi', 'article_type', 'lang', 'pdf_url', 'keywords', 'review_url',\n",
    "              'code_url', 'volume', 'issue', 'year', 'abstract', 'easy', 'difficult', 'gs_citations',\n",
    "              'gs_scholar_url', 'original_pdf_url', 'original_article_url', 'reason_for_easiness',\n",
    "              'reason_for_difficulty', 'limitations_results', 'scope_of_reproducibility',\n",
    "              'original_abstract', 'orig_art_sciparse_full_text', 'orig_art_pdfminer_full_text',\n",
    "              'original_sections', 'no_hyp', 'no_alg', 'no_images', 'no_equations', 'no_tables',\n",
    "              'is_meth_pres', 'is_intro_pres', 'link_to_code_available', 'mean_readability',\n",
    "              'hyp_available_in_text', 'easiness_longform', 'difficult_longform',\n",
    "              'list_for_limitations', 'list_for_diff', 'list_for_easiness', 'more_than_one_easy']\n",
    "\"\"\"\n",
    "\n",
    "# metadata\n",
    "meta_data_columns = [\"doi\", \"title\", \"review_url\", \"easy\", \"difficult\",\n",
    "                     \"scope_of_reproducibility\", \"reason_for_easiness\", \"reason_for_difficulty\"]\n",
    "print(f\"Rescience Metadata columns of interest: {meta_data_columns}\")\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "# sneak peak of the data\n",
    "print(rescience.select(meta_data_columns).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRyiDVFcnHND"
   },
   "source": [
    "### 2. Prepare Preference Dataset for **Group Relative Policy Optimization(GRPO)**\n",
    "\n",
    "<img src=\"https://github.com/akhilpandey95/LLMSciSci/blob/main/media/LLMSciSci_GRPO_dataset.png?raw=true\" width=700 height=450>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Icxp0qrqnIJc",
    "outputId": "885c2676-0cbf-48e7-8c9b-ebcd0928da14"
   },
   "outputs": [],
   "source": [
    "# @title 2.1 Load raw preference data from `GPT`, `Gemini` and `Llama` responses\n",
    "# read the gemini labelling data\n",
    "gemini_effortly = pl.read_csv(\"./data/gemini_effortly_labels_gamma.csv\")\n",
    "\n",
    "# read the gpt labelling data\n",
    "gpt_effortly = pl.read_csv(\"./data/gpt4_effortly_labels_beta.csv\")\n",
    "\n",
    "# read the llama labelling data\n",
    "llama_effortly = pl.read_csv(\"./data/llama3_effortly_labels_beta.csv\")\n",
    "\n",
    "# show a preview of the response(s)\n",
    "print(\"---------------------------\")\n",
    "print(f\"Response from gemini\")\n",
    "print(gemini_effortly.select(\"easy_gemini_response\")[0].item())\n",
    "print(\"---------------------------\")\n",
    "print(f\"Response from gpt\")\n",
    "print(gpt_effortly.select(\"easy_gpt_response\")[0].item())\n",
    "print(\"---------------------------\")\n",
    "print(f\"Response from llama\")\n",
    "print(llama_effortly.select(\"easy_llama3_response\")[0].item())\n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "30QT855oQpfe"
   },
   "outputs": [],
   "source": [
    "# @title 2.2 helper functions to generate synthetic COT using `Gemini`\n",
    "# structured response for synthetic COT traces\n",
    "class ReproEffortCOT(BaseModel):\n",
    "    cot_trace: str\n",
    "\n",
    "# main routine function to generate synthetic COT\n",
    "def easy_diff_COT_generate(text: str, label: str, task: str):\n",
    "    \"\"\"\n",
    "    Processing routine to take a raw text and\n",
    "    label to generate and return synthetic COT\n",
    "    from reasoning models.\n",
    "\n",
    "    --------------------\n",
    "    Parameters:\n",
    "        arg1 | text: str\n",
    "            Raw text from \"What was easy ?\" or \"What was difficult ?\" sections\n",
    "        arg2 | label: str\n",
    "            Parsed ground truth label for \"What was easy ?\" or \"What was difficult ?\" sections\n",
    "        arg3 | task: str\n",
    "            Task type \"easy\" or \"difficult\"\n",
    "\n",
    "    --------------------\n",
    "    Returns:\n",
    "        Dictionary\n",
    "            dict\n",
    "    \"\"\"\n",
    "    # default system prompt\n",
    "    sys_prompt = \"\"\"\n",
    "    You are a intelligent research assistant working on elaborating\n",
    "    your thought process about evaluating the effort of reproducing\n",
    "    academic articles.\n",
    "    \"\"\"\n",
    "\n",
    "    # default rules\n",
    "    rules = \"\"\"\n",
    "    **Rules:**\n",
    "    1. PUT ALL of your reasoning trace within steps, like step 1, step 2 etc.\n",
    "    2. The generated reasoning trace must connect how humans arrived at the \"Ground truth\" sections.\n",
    "    3. Ideally, the generated chain of thought should look like internal monologue and not robotic steps.\n",
    "    \"\"\"\n",
    "\n",
    "    # init user prompt\n",
    "    user_prompt, input_text, response_data = None, None, None\n",
    "\n",
    "    # what was easy ?\n",
    "    if task == \"easy\":\n",
    "        # init user prompt for the task\n",
    "        user_prompt=\"\"\"\n",
    "        **Task:** You are given brief descriptions that made it easy for researcher\n",
    "        to reproduce original articles. Your goal is to analyze the brief description\n",
    "        and generate reasoning chains of thought that helped people classify them\n",
    "        into one or more from the following five categories, which include:\n",
    "\n",
    "        1. Availability of Code\n",
    "        2. Supporting Artifacts\n",
    "        3. Readability of Full Text\n",
    "        4. Experimental Setup or Environment\n",
    "        5. Cannot extract concrete factors that Eased Reproducibility.\n",
    "        \"\"\"\n",
    "\n",
    "        # init the prompt\n",
    "        input_text = \"\"\"\n",
    "        **What was easy:**\n",
    "        ```plaintext\n",
    "        EASY_DESCRIPTION\n",
    "        ```\n",
    "\n",
    "        **Ground truth:**\n",
    "        ```plaintext\n",
    "        EASY_LABEL\n",
    "        ```\n",
    "        \"\"\"\n",
    "\n",
    "        # replace inputs\n",
    "        input_text = input_text.replace(\"EASY_DESCRIPTION\", text).replace(\"EASY_LABEL\", label)\n",
    "    else:\n",
    "        # init user prompt for the task\n",
    "        user_prompt=\"\"\"\n",
    "        **Task:** You are given brief descriptions that made it difficult for\n",
    "        researcher to reproduce original articles. Your goal is to analyze the\n",
    "        brief description and generate reasoning chains of thought that helped\n",
    "        people classify them into one or more from the following five categories,\n",
    "        which include:\n",
    "\n",
    "        1. Missing Algorithm step or Architecture details\n",
    "        2. Missing nuance details\n",
    "        3. Unclear notation or documentation in the codebase\n",
    "        4. Insufficient Math/Equations\n",
    "        5. Cannot extract concrete factors that made it difficult for reproducibility.\n",
    "        \"\"\"\n",
    "\n",
    "        # init the prompt\n",
    "        input_text = \"\"\"\n",
    "        **What was difficult:**\n",
    "        ```\n",
    "        DIFF_DESCRIPTION\n",
    "        ```\n",
    "\n",
    "        **Ground truth:**\n",
    "        ```plaintext\n",
    "        DIFF_LABEL\n",
    "        ```\n",
    "        \"\"\"\n",
    "\n",
    "        # replace inputs\n",
    "        input_text = input_text.replace(\"DIFF_DESCRIPTION\", text).replace(\"DIFF_LABEL\", label)\n",
    "\n",
    "    # choose the model\n",
    "    client = genai.Client(api_key=\"AIzaSyBwbNneEfuXuLO4qrOw2DWByDDFKH2aBeU\")\n",
    "\n",
    "    # config to run model\n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "            max_output_tokens=2048,\n",
    "            safety_settings=[\n",
    "                types.SafetySetting(\n",
    "                    category=\"HARM_CATEGORY_CIVIC_INTEGRITY\",\n",
    "                    threshold=\"OFF\",\n",
    "                ),\n",
    "                types.SafetySetting(\n",
    "                    category=\"HARM_CATEGORY_HARASSMENT\",\n",
    "                    threshold=\"OFF\",\n",
    "                ),\n",
    "                types.SafetySetting(\n",
    "                    category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                    threshold=\"OFF\",\n",
    "                ),\n",
    "                types.SafetySetting(\n",
    "                    category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                    threshold=\"OFF\",\n",
    "                ),\n",
    "                types.SafetySetting(\n",
    "                    category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                    threshold=\"OFF\",\n",
    "                ),\n",
    "            ],\n",
    "            response_mime_type=\"text/plain\",\n",
    "        )\n",
    "\n",
    "    # capture prompt response from Gemini\n",
    "    response = client.models.generate_content(model = 'gemini-2.0-flash-thinking-exp-01-21', \\\n",
    "                                              contents = sys_prompt + user_prompt + input_text + rules, \\\n",
    "                                              config=generate_content_config\n",
    "                                              )\n",
    "\n",
    "    # check if the response could be fetched\n",
    "    if not response.text:\n",
    "        # throw an exception\n",
    "        raise ValueError(\"[ERR]easy_diff_COT_generate: Empty response from the model\")\n",
    "\n",
    "    # update the response\n",
    "    response_data = response.text.strip()\n",
    "\n",
    "    # return the response\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QnZBqUlO_99-",
    "outputId": "b8a25788-ee74-4960-fb03-217295fa9c5c"
   },
   "outputs": [],
   "source": [
    "# @title 2.3 Run `easy_diff_COT_generate()`, pickle results\n",
    "\n",
    "# init dict to store results\n",
    "results = defaultdict(dict)\n",
    "records = []\n",
    "\n",
    "\"\"\"\n",
    "- run easy_diff_COT_generate() on all entries in gpt_effortly dataframe\n",
    "- save the results into a pickled object and save it to `./data/`\n",
    "\"\"\"\n",
    "# # set the keys for processing\n",
    "# task_types = [\"easy\", \"difficult\"]\n",
    "# chosen_types = [\"easy_gpt_response\", \"diff_gpt_response\"]\n",
    "\n",
    "# # iterate over all of the samples in easy\n",
    "# for idx in tqdm(range(len(gpt_effortly))):\n",
    "\n",
    "#     for i, task in enumerate(task_types):\n",
    "#         # get the text\n",
    "#         text = gpt_effortly.select(task)[idx].item()\n",
    "#         label = gpt_effortly.select(chosen_types[i])[idx].item()\n",
    "\n",
    "#         # generate reasoning trace\n",
    "#         cot = easy_diff_COT_generate(text, label, task)\n",
    "\n",
    "#         # store the results\n",
    "#         records.append({\n",
    "#             \"idx\": (2*idx) + i,\n",
    "#             \"doi\": gpt_effortly.select(\"doi\")[idx].item(),\n",
    "#             \"task\": task,\n",
    "#             \"label\": label,\n",
    "#             \"cot\": cot\n",
    "#         })\n",
    "\n",
    "# load the pickle file\n",
    "with open(\"./data/easy_diff_COT.pkl\", \"rb\") as f:\n",
    "    records = pickle.load(f)\n",
    "\n",
    "# shape of the records\n",
    "print(f\"# of records: {len(records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "hQY_-qonWnzI"
   },
   "outputs": [],
   "source": [
    "# @title 2.4 helper class to build the $D_{ReproEffortDataset}$ preference dataset\n",
    "\n",
    "# helper function to load/initalize the prompt\n",
    "def process_prompt(raw_text, tokenizer, device, task=\"easy\", prompt_type=\"prompt\"):\n",
    "    \"\"\"\n",
    "    Given raw input text generate a prompt that will\n",
    "    be supplied to a preference dataset loader.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    arg1 | raw_text: str\n",
    "        Raw input text without prompt template\n",
    "    arg2 | tokenizer: transformers.tokenization_utils_fast.PreTrainedTokenizerFast\n",
    "        Tokenizer from the model\n",
    "    arg3 | device: str\n",
    "        Device name for the inputs and attention masks to sit on\n",
    "    arg4 | task: str[OPTIONAL]\n",
    "        Task type \"What was easy ?\" or \"What was difficult ?\"\n",
    "    arg5 | prompt_type: str[OPTIONAL]\n",
    "        String flag to be applied at the top of messages to create \"prompt\"\n",
    "        or \"completions\" chat responses for the GRPO preference dataset\n",
    "\n",
    "    Returns\n",
    "    ------------\n",
    "        Text\n",
    "    \"\"\"\n",
    "    # init\n",
    "    prompt = None\n",
    "    messages = []\n",
    "    add_generation_prompt = True\n",
    "    sys_prompt, user_prompt, input_text = None, None, None\n",
    "\n",
    "    # init system prompt available\n",
    "    sys_prompt = \"\"\"You are a research assistant working on understanding the\n",
    "    spectrum of outputs researchers outline when reproducing\n",
    "    academic articles.\n",
    "    Respond in the following format:\n",
    "    <think>\n",
    "    ...\n",
    "    </think>\n",
    "    <label>\n",
    "    ...\n",
    "    </label>\n",
    "    \"\"\"\n",
    "\n",
    "    # what was easy ?\n",
    "    if task == \"easy\":\n",
    "      # init user prompt for the task\n",
    "      user_prompt=\"\"\"**Task:** You are given brief descriptions that made it easy for researcher\n",
    "      to reproduce original articles. Your goal is to analyze the brief description\n",
    "      and classify them into one or more from the following five categories,\n",
    "      which include:\n",
    "\n",
    "      1. Availability of Code\n",
    "      2. Supporting Artifacts\n",
    "      3. Readability of Full Text\n",
    "      4. Experimental Setup or Environment\n",
    "      5. Cannot extract concrete factors that Eased Reproducibility.\n",
    "      \"\"\"\n",
    "\n",
    "      # init the prompt\n",
    "      input_text = \"\"\"\n",
    "      **What was easy:**\n",
    "      <text>\n",
    "      EASY_DESCRIPTION\n",
    "      </text>\n",
    "      \"\"\"\n",
    "    else:\n",
    "      # init user prompt for the task\n",
    "      user_prompt=\"\"\"**Task:** You are given brief descriptions that made it difficult for\n",
    "      researcher to reproduce original articles. Your goal is to analyze the\n",
    "      description and classify them into one or more from the following\n",
    "      five categories:\n",
    "\n",
    "      1. Missing Algorithm step or Architecture details\n",
    "      2. Missing nuance details\n",
    "      3. Unclear notation or documentation in the codebase\n",
    "      4. Insufficient Math/Equations\n",
    "      5. Cannot extract concrete factors that made it difficult for reproducibility.\n",
    "      \"\"\"\n",
    "\n",
    "      # init the prompt\n",
    "      input_text = \"\"\"\n",
    "      **What was difficult:**\n",
    "      <text>\n",
    "      DIFF_DESCRIPTION\n",
    "      </text>\n",
    "      \"\"\"\n",
    "\n",
    "    # apply chat template on the chosen/rejected response\n",
    "    if prompt_type == \"completion\":\n",
    "      # init the completion prompt\n",
    "      input_text = \"<think>\\nCOT_REASONING_TRACE\\n</think>\\n<label>\\nGROUND_TRUTH_LABEL\\n</label>\"\n",
    "\n",
    "      # add the input text\n",
    "      input_text = input_text.replace(\"COT_REASONING_TRACE\", raw_text[0]).replace(\"GROUND_TRUTH_LABEL\", raw_text[1])\n",
    "\n",
    "      # gemma needs user/assistant and not just assistant\n",
    "      if isinstance(tokenizer, transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast):\n",
    "          # set the chosen response for the preferences\n",
    "          messages.append([{\"role\": \"user\", \"content\": \"\"}, {\"role\": \"assistant\", \"content\": input_text}])\n",
    "      else:\n",
    "          # set the chosen response for the preferences\n",
    "          messages.append([{\"role\": \"assistant\", \"content\": input_text}])\n",
    "\n",
    "      # apply prompt template\n",
    "      add_generation_prompt=False\n",
    "\n",
    "      # apply prompt and remove the system prompt\n",
    "      prompt = tokenizer.apply_chat_template(messages, \\\n",
    "                                            tokenize=False, \\\n",
    "                                            use_system_prompt=add_generation_prompt, \\\n",
    "                                            add_generation_prompt=add_generation_prompt)\n",
    "      # prompt = messages\n",
    "    else:\n",
    "      # adjust and replace EASY_DESCRIPTION or DIFF_DESCRIPTION based on task\n",
    "      if task == \"easy\":\n",
    "        input_text = input_text.replace(\"EASY_DESCRIPTION\", raw_text)\n",
    "      else:\n",
    "        input_text = input_text.replace(\"DIFF_DESCRIPTION\", raw_text)\n",
    "\n",
    "      # set the prompt for the preferences\n",
    "      messages.append([\n",
    "          {\"role\": \"system\", \"content\": sys_prompt},\n",
    "          {\"role\": \"user\", \"content\": user_prompt + input_text}\n",
    "      ])\n",
    "\n",
    "      # apply prompt template\n",
    "      prompt = tokenizer.apply_chat_template(messages, \\\n",
    "                                            tokenize=False, \\\n",
    "                                            add_generation_prompt=add_generation_prompt)\n",
    "      # prompt = messages\n",
    "\n",
    "    # return the processed prompt\n",
    "    return prompt\n",
    "\n",
    "# utility class to create the preference dataset\n",
    "class ReproEffortPrefDataset:\n",
    "    def __init__(self, raw_data, tokenizer, records, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Given raw text prepare preference dataset.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        arg1 | raw_data: polars.DataFrame or pandas.DataFrame or List[dict]\n",
    "            ML reproducibility challenge data processed with the following columns:\n",
    "              - \"easy\": Raw prompt text for the easy task.\n",
    "              - \"easy_gpt_response\": Chosen response for the easy task.\n",
    "              - \"easy_llama3_response\": Rejected response for the easy task.\n",
    "              - \"difficult\": Raw prompt text for the difficult task.\n",
    "              - \"difficult_gpt_response\": Chosen response for the difficult task.\n",
    "              - \"diff_llama3_response\": Rejected response for the difficult task.\n",
    "        arg2 | tokenizer: transformers.tokenization_utils_fast.PreTrainedTokenizerFast\n",
    "            Tokenizer from the model to apply chat template.\n",
    "        arg3 | device: str[OPTIONAL]\n",
    "            Device name (e.g., \"cpu\" or \"cuda\") for processing.\n",
    "\n",
    "        Returns\n",
    "        ------------\n",
    "            Text\n",
    "        \"\"\"\n",
    "        # polars df ? convert it to a pandas DataFrame.\n",
    "        if hasattr(raw_data, \"to_pandas\"):\n",
    "            raw_data = raw_data.to_pandas()\n",
    "        # pd df ? convert to list of dicts\n",
    "        if isinstance(raw_data, pd.DataFrame):\n",
    "            self.raw_data = raw_data.to_dict(\"records\")\n",
    "        elif isinstance(raw_data, list):\n",
    "            self.raw_data = raw_data\n",
    "        else:\n",
    "            raise ValueError(\"ERR[ReproEffortPrefDataset]: Unsupported raw_data type; must be a pl.DataFrame, pd.DataFrame, or list[dict].\")\n",
    "\n",
    "        # init default arguments\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    # helper function to build the dataset object\n",
    "    def build_dataset(self, test_size=0.2, seed=2025):\n",
    "        \"\"\"\n",
    "        Build a unified preference dataset with\n",
    "        \"What was easy ?\" and \"What was difficult ?\" texts.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        arg1 | test_size: float[OPTIONAL]\n",
    "            Set the size of the test set, defaults to 0.2 or 20% of the dataset.\n",
    "        arg2 | seed: int[OPTIONAL]\n",
    "            Seed parameter for reproducibility, defaults to 2025.\n",
    "\n",
    "        Returns\n",
    "        ------------\n",
    "            Dictionary {\"prompt\": str, \"completion\": str}\n",
    "        \"\"\"\n",
    "        # inti list to store results\n",
    "        results = []\n",
    "\n",
    "        # set the keys for processing\n",
    "        task_types = [\"easy\", \"difficult\"]\n",
    "        chosen_types = [\"y_easy_gpt4\", \"y_diff_gpt4\"]\n",
    "\n",
    "        # iterate and combine \"easy\" and \"difficult\" tasks\n",
    "        for idx, sample in enumerate(self.raw_data):\n",
    "            # procdess for each task\n",
    "            for i, task in enumerate(task_types):\n",
    "                # set the prompt\n",
    "                prompt = process_prompt(sample[task], self.tokenizer, self.device, task=task, prompt_type=\"prompt\")[0]\n",
    "\n",
    "                # set the completions\n",
    "                chosen = process_prompt((records[(2*idx) + i][\"cot\"], sample[chosen_types[i]]), \\\n",
    "                                        self.tokenizer, self.device, task=task, prompt_type=\"completion\")[0]\n",
    "\n",
    "                # append the records\n",
    "                results.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"completion\": chosen,\n",
    "                    \"task\": task,\n",
    "                    \"type\": chosen_types[i],\n",
    "                    \"idx\": sample[\"doi\"]\n",
    "                })\n",
    "\n",
    "        # merge records\n",
    "        combined_data = {\n",
    "            \"prompt\": [r[\"prompt\"] for r in results],\n",
    "            \"completion\": [r[\"completion\"] for r in results],\n",
    "            \"task\": [r[\"task\"] for r in results],\n",
    "            \"type\": [r[\"type\"] for r in results],\n",
    "            \"idx\": [r[\"idx\"] for r in results]\n",
    "        }\n",
    "\n",
    "        # init hf dataset and perform train/test split.\n",
    "        dataset = Dataset.from_dict(combined_data)\n",
    "\n",
    "        # shuffle the datset\n",
    "        dataset = dataset.shuffle()\n",
    "\n",
    "        # train test split\n",
    "        dataset_split = dataset.train_test_split(test_size=test_size, seed=seed)\n",
    "\n",
    "        # return final dataset\n",
    "        return dataset_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GysSQyMnp6r"
   },
   "source": [
    "### 3. API & Local Models setup\n",
    "\n",
    "For the commercial models you would need to setup your account and obtainan API key to run some of the experiments in this notebook.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Pre-requisites for commercial models**\n",
    "<br>\n",
    "**OpenAI**: https://platform.openai.com/settings/organization/api-keys\n",
    "<hr>\n",
    "\n",
    "**Pre-requisites for local models**\n",
    "<br>\n",
    "The experiments and widgets in the notebook require `data/` and `models/`. Since `data/` is loaded, we need model weights which can be downloaded here:\n",
    "- [Models](https://drive.google.com/drive/folders/1aNT1SNA7Lz9kMgt5p1yGWST1T6D2Dmbd?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mXssSsHhnogU",
    "outputId": "cdf139c3-7cfe-4e0a-e011-a010b336ef4b"
   },
   "outputs": [],
   "source": [
    "# @title 3.1 Local model Catalog\n",
    "!ls -lah models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "EqfYxA-Pxp04"
   },
   "outputs": [],
   "source": [
    "# @title 3.2 Load model client or model-tokenizer pair\n",
    "# helper function to load/initalize the model\n",
    "def load_model(model_name, device):\n",
    "    \"\"\"\n",
    "    Given a model path, load tokenizer-model\n",
    "    pair and return the objects tagged to the\n",
    "    given device (cpu/cuda)\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    arg1 | model_name: str\n",
    "        Use model catalog to load local model weights\n",
    "    arg2 | device: str\n",
    "        Hardware acceleration, defaults to \"cpu\" if any errors arise\n",
    "\n",
    "    Returns\n",
    "    ------------\n",
    "        Tuple(AutoModel, AutoTokenizer) for local (model_client, model_name)\n",
    "    \"\"\"\n",
    "    # accelerator centric attention implementtion\n",
    "    attn_implementation = \"sdpa\"\n",
    "\n",
    "    # device for acceleration\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        # attn_implementation = \"flash_attention_2\"\n",
    "    elif torch.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    # local models\n",
    "    local_models = [\"llama3.2-1b\", \"llama3.2-3b\", \"llama3.1-8b\", \"qwen2.5-1.5b\", \"r1-distill-qwen-1.5b\"]\n",
    "\n",
    "    # pathlib for models\n",
    "    model_path = pathlib.Path(\"/content/drive/MyDrive/CSSI/Lecture\")\n",
    "\n",
    "    # set the model-id\n",
    "    model_catalog = {\n",
    "        \"llama-3.2-1b\": model_path/f\"models/Llama3.2-1B-Instruct/\",\n",
    "        \"llama-3.2-3b\": model_path/f\"models/Llama3.2-3B-Instruct/hf/\",\n",
    "        \"llama-3.1-8b\": model_path/f\"models/Meta-Llama-3.1-8B-Instruct/hf/\",\n",
    "        \"gemma-3-1b\": model_path/f\"models/gemma-3-1b-it/\",\n",
    "        \"gemma-3-4b\": model_path/f\"models/gemma-3-4b-it/\",\n",
    "        \"qwen-2.5-1.5b\": model_path/f\"models/Qwen2.5-1.5B-Instruct/\"\n",
    "    }\n",
    "\n",
    "    # set a model-id\n",
    "    model_id = model_catalog[model_name]\n",
    "\n",
    "    # log\n",
    "    print(\"----------------------------------\")\n",
    "    print(f\"Using {device} to load {model_id}\")\n",
    "    print(\"----------------------------------\")\n",
    "\n",
    "    # get model-tokenizer pair\n",
    "    start = time.time()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    tokenizer.padding_side  = 'left'\n",
    "\n",
    "    # based on model size switch quantization config\n",
    "    if model_name == \"llama3.1-70b\" or model_name == \"r1-distill-llama-70b\":\n",
    "        # 4-bit quantization config\n",
    "        bnb_4bit = BitsAndBytesConfig(\n",
    "          load_in_4bit=True,\n",
    "          bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "          bnb_4bit_quant_storage=torch.bfloat16\n",
    "        )\n",
    "\n",
    "        # 4 bit quantization\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, \\\n",
    "                                                   quantization_config=bnb_4bit, \\\n",
    "                                                   trust_remote_code=True, \\\n",
    "                                                   low_cpu_mem_usage=True, \\\n",
    "                                                   attn_implementation=attn_implementation, \\\n",
    "                                                   device_map=device)\n",
    "    elif model_name == \"gemma-3-4b\" or model_name == \"gemma-3-1b\":\n",
    "        model = Gemma3ForCausalLM.from_pretrained(model_id, \\\n",
    "                                              trust_remote_code=True, \\\n",
    "                                              torch_dtype=torch.bfloat16, \\\n",
    "                                              low_cpu_mem_usage=True, \\\n",
    "                                              attn_implementation=attn_implementation, \\\n",
    "                                              device_map=device)\n",
    "    else:\n",
    "      # 4-bit quantization config\n",
    "      bnb_4bit = BitsAndBytesConfig(\n",
    "          load_in_4bit=True,\n",
    "          bnb_4bit_use_double_quant=True,\n",
    "          bnb_4bit_quant_type=\"nf4\",\n",
    "          bnb_4bit_compute_dtype=torch.bfloat16\n",
    "      )\n",
    "\n",
    "      # load bfloat16 weights\n",
    "      model = AutoModelForCausalLM.from_pretrained(model_id, \\\n",
    "                                                   trust_remote_code=True, \\\n",
    "                                                   torch_dtype=torch.bfloat16, \\\n",
    "                                                   low_cpu_mem_usage=True, \\\n",
    "                                                   attn_implementation=attn_implementation, \\\n",
    "                                                   device_map=device)\n",
    "\n",
    "    # is it a llama tokenizer ?\n",
    "    if \"llama\" in model_name:\n",
    "        # pad token if needed\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"<|finetune_right_pad_id|>\"})\n",
    "        print(f\"Setting <|finetune_right_pad_id|> token for {model_id}\")\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        # llama prompt template\n",
    "        llama_template = r\"\"\"\n",
    "        {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
    "        \"\"\"\n",
    "\n",
    "        # set the chat template\n",
    "        tokenizer.chat_template = llama_template\n",
    "\n",
    "    # gemma eos token\n",
    "    if \"gemma\" in model_name:\n",
    "        # set the EOS tokenid\n",
    "        tokenizer.eos_token_id = tokenizer.encode(\"<end_of_turn>\")[0]\n",
    "\n",
    "        # set the chat template\n",
    "        gemma_template = r\"\"\"\n",
    "        {% if messages[0]['role'] == 'system' %}\n",
    "            {% set system_message = messages[0]['content'] | trim + '\\n\\n' %}\n",
    "            {% set messages = messages[1:] %}\n",
    "        {% else %}\n",
    "            {% set system_message = '' %}\n",
    "        {% endif %}\n",
    "\n",
    "        {% for message in messages %}\n",
    "            {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n",
    "                {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n",
    "            {% endif %}\n",
    "\n",
    "            {% if loop.index0 == 0 %}\n",
    "                {% set content = system_message + message['content'] %}\n",
    "            {% else %}\n",
    "                {% set content = message['content'] %}\n",
    "            {% endif %}\n",
    "\n",
    "            {% if (message['role'] == 'assistant') %}\n",
    "                {% set role = 'model' %}\n",
    "            {% else %}\n",
    "                {% set role = message['role'] %}\n",
    "            {% endif %}\n",
    "\n",
    "            {{ '<start_of_turn>' + role + '\\n' + content | trim + '<end_of_turn>\\n' }}\n",
    "        {% endfor %}\n",
    "\n",
    "        {% if add_generation_prompt %}\n",
    "            {{'<start_of_turn>model\\n'}}\n",
    "        {% endif %}\n",
    "        \"\"\"\n",
    "\n",
    "        # set the chat template\n",
    "        tokenizer.chat_template = gemma_template\n",
    "\n",
    "    # load time\n",
    "    end = time.time()\n",
    "    print(f\"Model-tokenizer Load Time:, {end - start} seconds\")\n",
    "    print(\"----------------------------------\")\n",
    "\n",
    "    # return the pair\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vzY4Ega61vBi",
    "outputId": "a8145b19-7a7c-4979-a026-842bd41c9f6b"
   },
   "outputs": [],
   "source": [
    "# @title 3.3 Load policy model for $\\pi_{LLMSciSci}$\n",
    "\n",
    "# set model and device name\n",
    "# device = \"cpu\"\n",
    "device = \"cuda\"\n",
    "model_name = \"gemma-3-1b\"\n",
    "# model_name = \"qwen-2.5-1.5b\"\n",
    "# model_name = \"llama-3.2-1b\"\n",
    "\n",
    "# load the model for policy update\n",
    "model, tokenizer = load_model(model_name, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXX3Srn4s8kz"
   },
   "source": [
    "### 4. Preference tuning via **Group Relative Policy Optimization(GRPO):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66L90UJitIaz",
    "outputId": "118fac9a-9b1b-49db-d286-52b761d9d421"
   },
   "outputs": [],
   "source": [
    "# @title 4.1 Init `ReproEffortPrefDataset` dataset object, train/test split\n",
    "\n",
    "# final shape of the raw data\n",
    "print(f\"-----------------------------------\")\n",
    "print(f\"Shape of raw_data: {gpt_effortly.shape}\")\n",
    "print(f\"-----------------------------------\")\n",
    "print(\"Columns in raw_data:\")\n",
    "print(f\"-----------------------------------\")\n",
    "pprint(gpt_effortly.columns)\n",
    "print(f\"-----------------------------------\")\n",
    "\n",
    "# convert raw data to pandas\n",
    "raw_data_pd = gpt_effortly.to_pandas()\n",
    "\n",
    "# create the preference dataset\n",
    "dataset_obj = ReproEffortPrefDataset(raw_data_pd, tokenizer, records, device=device)\n",
    "dataset = dataset_obj.build_dataset(test_size=0.2, seed=2025)\n",
    "print(f\"-----------------------------------\")\n",
    "print(f\"ReproEffortPrefDataset:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "f1p_hUx8tVzL"
   },
   "outputs": [],
   "source": [
    "# @title 4.2 Preview of the reasoning CoTs for \"What was easy ?\" and \"What was difficult ?\" tasks\n",
    "\n",
    "# print sample cut of the dataset\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Prompt for the preference dataset..\")\n",
    "print(dataset[\"train\"][-1][\"prompt\"])\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Reasoning:\")\n",
    "print(dataset[\"train\"][-1][\"completion\"])\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152,
     "referenced_widgets": [
      "0d96a09c7728435182dc7268c090d899",
      "b273502164ff4d4a8946f3b6c922a1e1",
      "bab1fdd8d53b42feacf8b2669fcb8c83",
      "6768710f6e2647deb3c957ec9e80bb27",
      "746a7e56e32147b198aeb507f65f8f0d",
      "15ea96b6ee064ee28b79a39831d692c5",
      "902b10d8fa5d494a87ed28102f17df3f",
      "e32c1c277cd14331abfd9ba48924a13d",
      "14140511d7d3420cb828031090febc4c",
      "b5897113524741bb88e4e4e64976e90d",
      "93babdc2456c4d58b4e2299efe868126",
      "7bcd57a7f2284330a8daf68025181957",
      "d0a322daa3b340e8979f758622ae4d92",
      "7bd12161274f4d918f77b538520d7682",
      "08654c67b28d41b39a1c9c5c755b5bb6",
      "7f51fd31477648efad201d9680ab2609",
      "a8233b74e41541f781a2d61aa762e24c",
      "285b6fe358df4fe9af94834e03bb27cd",
      "a6132dd3a7e3425eb24191afcbd118a2",
      "3c93f441f71c44e0a164bb3a77db866d",
      "3e8a92d1818b4275b17482dce8a371c9",
      "1aafa4c73cc849feae3779ff7397260b"
     ]
    },
    "id": "QIKa-QPezLOl",
    "outputId": "5eb83c9f-152f-4b60-dd64-c93cced5f2fb"
   },
   "outputs": [],
   "source": [
    "# @title 4.3 Tokenomics to decide `max_seq_length` and `prompt_length`\n",
    "# gather the train and test datasets\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# lets find the p95 length of the prompt\n",
    "prompt_length = int(np.percentile([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset[\"prompt\"]], 95))\n",
    "max_seq_length = int(np.percentile([len(tokenizer(x[\"prompt\"] + \\\n",
    "                                                  x[\"completion\"])[\"input_ids\"]) \\\n",
    "                                    for x in train_dataset], 95))\n",
    "\n",
    "# filter datasets to remove samples that are too long\n",
    "train_dataset = train_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"] + \\\n",
    "                                                             x[\"completion\"])[\"input_ids\"]) <= max_seq_length)\n",
    "test_dataset = test_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"] + \\\n",
    "                                                           x[\"completion\"])[\"input_ids\"]) <= max_seq_length)\n",
    "print(f\"len(train_dataset): {len(train_dataset)}\")\n",
    "print(f\"len(test_dataset): {len(test_dataset)}\")\n",
    "\n",
    "# # Up the lengths to next multiple of 2, why 2? Don't know\n",
    "prompt_length = ((prompt_length + 1) // 2) * 2\n",
    "max_seq_length = ((max_seq_length + 1) // 2) * 2\n",
    "print(f\"p95 prompt length: {prompt_length}\")\n",
    "print(f\"p95 prompt + chosen length: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "D0kiY0TZ0xVx"
   },
   "outputs": [],
   "source": [
    "# @title 4.4 Build rewards for $GRPO$\n",
    "import re\n",
    "from ast import literal_eval as le\n",
    "from sklearn.metrics import hamming_loss as hl\n",
    "\n",
    "# helper function for getting the label out\n",
    "def extract_text(text: str, tag: str) -> str:\n",
    "    label = text.split(\"<\" + tag + \">\")[-1]\n",
    "    label = label.split(\"</\" + tag + \">\")[0]\n",
    "    return label.strip()\n",
    "\n",
    "# helper function to check one hot format of the extracted label\n",
    "def is_valid_onehot(label_str: str, expected_length: int = 5) -> bool:\n",
    "    try:\n",
    "        label = le(label_str)\n",
    "        if isinstance(label, list) and len(label) == expected_length:\n",
    "            return all(isinstance(x, int) and x in (0, 1) for x in label)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "# hard reward function to check overall answer format\n",
    "def format_reward_func(completions, **kwargs):\n",
    "    # pattern = r\"(?s).*<think>.*?</think>\\s*<label>.*?</label>.*\"\n",
    "    pattern = r\"(?sm).*<think>.*?</think>\\s*<label>.*?</label>.*\"\n",
    "    matches = [re.match(pattern, content, re.DOTALL | re.MULTILINE) for content in completions]\n",
    "    return [1.0 if match else 0.0 for match in matches]\n",
    "\n",
    "# soft reward function to reward one hot labels\n",
    "def label_reward_func(completions, **kwargs):\n",
    "    # regex pattern to capture results\n",
    "    # pattern = r\"(?s).*<think>.*?</think>\\s*<label>.*?</label>.*\"\n",
    "    pattern = r\"(?sm).*<think>.*?</think>\\s*<label>.*?</label>.*\"\n",
    "\n",
    "    # init results\n",
    "    rewards = []\n",
    "\n",
    "    # match format reward\n",
    "    for content in completions:\n",
    "        try:\n",
    "            matched = re.match(pattern, content).string\n",
    "            extracted_responses = extract_text(matched, \"label\")\n",
    "            if is_valid_onehot(extracted_responses):\n",
    "                rewards.append(0.5)\n",
    "            else:\n",
    "                rewards.append(0.0)\n",
    "        except Exception:\n",
    "            rewards.append(0.0)\n",
    "\n",
    "    # return rewards\n",
    "    return rewards\n",
    "\n",
    "# stepwise label rewards function to encourage progress\n",
    "def stepwise_label_reward_func(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Stepwise reward:\n",
    "\n",
    "    1) +0.125 if there exists text within <label>...</label>.\n",
    "    2) +0.125 if that text consists only of 0's and 1's (ignoring brackets, commas, and whitespace).\n",
    "    3) +0.125 if the text starts with '[' and ends with ']'.\n",
    "    4) +0.625 if the text passes the is_valid_onehot() check.\n",
    "    \"\"\"\n",
    "\n",
    "    # This pattern ensures we have something in <think>...</think>\n",
    "    # followed by <label>...</label> in the content (dot matches newlines).\n",
    "    pattern = r\"(?sm).*<think>.*?</think>\\s*<label>.*?</label>.*\"\n",
    "\n",
    "    rewards = []\n",
    "    for content in completions:\n",
    "        total_reward = 0.0\n",
    "        try:\n",
    "            # Make sure the content at least matches the structure\n",
    "            # of having <label>...</label> after <think>...</think>\n",
    "            if re.match(pattern, content):\n",
    "                # Extract whatever is between <label> and </label>\n",
    "                label_str = extract_text(content, \"label\")\n",
    "\n",
    "                # 1) Check that we indeed have label text.\n",
    "                if label_str is not None and label_str.strip():\n",
    "                    total_reward += 0.125\n",
    "\n",
    "                    # 2) Check if the extracted text has only 0's and 1's\n",
    "                    #    (ignoring brackets, commas, and whitespace).\n",
    "                    stripped_str = re.sub(r'[\\[\\],\\s]', '', label_str)\n",
    "                    if re.fullmatch(r'[01]+', stripped_str):\n",
    "                        total_reward += 0.125\n",
    "\n",
    "                    # 3) Check if the label_str starts with '[' and ends with ']'\n",
    "                    if label_str.startswith('[') and label_str.endswith(']'):\n",
    "                        total_reward += 0.125\n",
    "\n",
    "                    # 4) Check if it passes the is_valid_onehot() condition\n",
    "                    if is_valid_onehot(label_str):\n",
    "                        total_reward += 0.625\n",
    "\n",
    "            # accrue rewards\n",
    "            rewards.append(total_reward)\n",
    "        except Exception:\n",
    "            # If something failed, we just assign zero\n",
    "            rewards.append(0.0)\n",
    "\n",
    "    # return rewards\n",
    "    return rewards\n",
    "\n",
    "# reward function to check correctness through hamming loss of the generated label\n",
    "def hamming_loss_label_reward(prompts, completions, task, ltype, dois, **kwargs):\n",
    "    # init results\n",
    "    prompt_contents, completion_contents, rewards = [], [], []\n",
    "\n",
    "    # regex pattern to capture results\n",
    "    # pattern = r\"(?s).*<think>.*?</think>\\s*<label>.*?</label>.*\"\n",
    "    pattern = r\"(?sm).*<think>.*?</think>\\s*<label>.*?</label>.*\"\n",
    "\n",
    "    # iterate over all samples\n",
    "    for prompt, completion, t, lt, doi in zip(prompts, completions, task, ltype, dois):\n",
    "        prompt_contents.append(prompt)\n",
    "        completion_contents.append(completion)\n",
    "\n",
    "        # get prompt raw text\n",
    "        prompt_content = prompt.split(\"<text>\")[-1]\n",
    "        prompt_content = prompt_content.split(\"</text>\")[0]\n",
    "\n",
    "        # get ground truth\n",
    "        y_true = gpt_effortly.filter(pl.col(\"doi\") == doi).select(lt).item()\n",
    "        completion_content = re.match(pattern, completion).string\n",
    "        y_hat = extract_text(completion_content, \"label\")\n",
    "        if is_valid_onehot(y_hat):\n",
    "            rewards.append(1 - hl( le(y_true), le(y_hat) ))\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "\n",
    "    # return rewards\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ONMYMyLtYX1q",
    "outputId": "44436b60-0472-46f9-af69-fb5c491e1e6b"
   },
   "outputs": [],
   "source": [
    "# @title 4.5 Testing the rewards on samples\n",
    "\n",
    "print(f\"Format rewards for five examples in training set:\")\n",
    "print(format_reward_func(completions=train_dataset[\"completion\"])[:5])\n",
    "print(f\"Label rewards for five examples in training set:\")\n",
    "print(label_reward_func(train_dataset[\"completion\"])[:5])\n",
    "print(f\"Hamming loss reward for five examples in training set:\")\n",
    "print(hamming_loss_label_reward(train_dataset[\"prompt\"], train_dataset[\"completion\"], train_dataset[\"task\"], train_dataset[\"type\"], train_dataset[\"idx\"])[:5])\n",
    "print(f\"Stepwise label reward for five examples in training set:\")\n",
    "print(stepwise_label_reward_func(train_dataset[\"completion\"])[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOvQGszmhXSb",
    "outputId": "5e939634-1d4f-4ed9-8706-dc02029b5987"
   },
   "outputs": [],
   "source": [
    "# cot length rewards function to encourage longer chain of thought\n",
    "def cot_trace_length_reward(completions, **kwargs):\n",
    "    # init rewards\n",
    "    rewards = []\n",
    "\n",
    "    # regex pattern\n",
    "    pattern = r\"(?sm).*<think>.*?</think>\\s*<label>.*?</label>.*\"\n",
    "\n",
    "    # iterate over the completions to calculate length reward\n",
    "    for completion in completions:\n",
    "        try:\n",
    "            # get prompt raw text\n",
    "            cot_content = re.match(pattern, completion).string\n",
    "            cot_content = extract_text(cot_content, \"think\")\n",
    "\n",
    "            # adjust reward for COT length\n",
    "            token_length = len(tokenizer(cot_content, padding=True, return_tensors=\"pt\")[\"input_ids\"][0])\n",
    "            rewards.append(token_length)\n",
    "        except Exception:\n",
    "            # empty rewards\n",
    "            rewards.append(0)\n",
    "\n",
    "    # return rewards\n",
    "    return rewards\n",
    "\n",
    "# combine stepwise rewards and cot length reward\n",
    "def cond_cot_steplabel_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Only add a length-based reward if the label meets a given validity threshold.\n",
    "    If the label score is below this threshold, length reward is 0.\n",
    "    final_reward = label_reward + alpha * length_reward (when label is valid)\n",
    "                 = label_reward (when label is invalid).\n",
    "\n",
    "    :param completions: list of generated outputs\n",
    "    :param alpha: scaling factor for length reward\n",
    "    :param validity_threshold: minimum label correctness score to unlock length reward\n",
    "    :param kwargs: any extra arguments your sub-reward functions need\n",
    "    :return: a list of float final rewards\n",
    "    \"\"\"\n",
    "    # set params\n",
    "    alpha = 0.1\n",
    "    validity_threshold = 1.0\n",
    "\n",
    "    # 1) Get the label reward for each completion\n",
    "    label_scores = stepwise_label_reward_func(completions, **kwargs)\n",
    "\n",
    "    # 2) Get the chain-of-thought length reward for each completion\n",
    "    length_scores = cot_trace_length_reward(completions, **kwargs)\n",
    "\n",
    "    # 3) Combine them conditionally\n",
    "    final_rewards = []\n",
    "    for label_score, length_score in zip(label_scores, length_scores):\n",
    "        if label_score >= validity_threshold:\n",
    "            # Label is valid enough -> add length reward\n",
    "            final_reward = label_score + alpha * length_score\n",
    "        else:\n",
    "            # Label isn't valid -> no length reward\n",
    "            final_reward = label_score\n",
    "        final_rewards.append(final_reward)\n",
    "\n",
    "    return final_rewards\n",
    "\n",
    "print(f\"COT trace length completion reward:\")\n",
    "print(cot_trace_length_reward(train_dataset[\"completion\"])[:5])\n",
    "\n",
    "print(f\"Combining COT length reward with Stepwise label reward:\")\n",
    "print(cond_cot_steplabel_reward(train_dataset[\"completion\"])[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h7q1AejC2d_M",
    "outputId": "d488e312-b85b-4a58-83c7-3f4b538d4c0a"
   },
   "outputs": [],
   "source": [
    "# @title 4.6 Ablations and safety testing to mitigate reward hacking\n",
    "a = \"\"\"\n",
    "-----------------------------------\n",
    "Reasoning:\n",
    "<|im_start|>system\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "<think>\n",
    "```\n",
    "step 1: Okay, let's break down this description to see why it's considered easy to reproduce.  First sentence: \"The original paper is well‐structured and easy to follow, with the principal ideas behind the proposed algorithms being very intuitive.\"  Hmm, \"well-structured and easy to follow\" clearly speaks to how readable the paper is.  It's not about code or data yet, but how the paper itself is written.  This sounds like it directly addresses category 3, 'Readability of Full Text'.  Intuitive algorithms also contribute to understandability, reinforcing this point. So, for Readability, it's a definite 'Yes'.\n",
    "\n",
    "step 2: Moving to the second sentence: \"Additionally, the datasets used in the ex‐ periments are publicly available, small in size, and the authors provide their code on GitHub.\"  Ah, datasets and code! These are tangible things needed for reproduction beyond just reading the paper.  \"Datasets publicly available\" is great.  Publicly available datasets are definitely 'Supporting Artifacts' (category 2).  The fact they are \"small in size\" is an added bonus for ease of use, but the key is their availability.  So, Supporting Artifacts gets a 'Yes'.\n",
    "\n",
    "step 3:  And then, \"authors provide their code on GitHub.\"  This is super explicit.  'Availability of Code' (category 1) is a direct match.  GitHub is a common platform for sharing code, making it easily accessible.  So, 'Availability of Code' is also a 'Yes'.\n",
    "\n",
    "step 4: Now, let's think about 'Experimental Setup or Environment' (category 4). The description mentions datasets and code, which are components of an experiment, but it doesn't actually describe *how* the experiment was set up, what environment was used (software versions, hardware, specific settings beyond the code itself).  It's more about the *inputs* (data) and the *implementation* (code) being available, rather than a detailed description of the experimental process or environment itself in the description provided.  So, based on *this* description, I can't say there's information specifically easing reproduction related to experimental setup *as a distinct category*. Therefore, for 'Experimental Setup or Environment', it's a 'No'.\n",
    "\n",
    "step 5: Finally, 'Cannot extract concrete factors that Eased Reproducibility' (category 5).  Wait a minute, we've already extracted several concrete factors!  We found 'Readability of Full Text', 'Availability of Code', and 'Supporting Artifacts (datasets)'.  These are all very concrete reasons why this paper is easy to reproduce based on the description.  So, it's definitely *not* the case that we cannot extract concrete factors.  Therefore, for 'Cannot extract concrete factors that Eased Reproducibility', it's a 'No'.\n",
    "\n",
    "step 6: Let me summarize.  Based on the description:\n",
    "- 'Availability of Code': Yes (code on GitHub)\n",
    "- 'Supporting Artifacts': Yes (publicly available datasets)\n",
    "- 'Readability of Full Text': Yes (well-structured, easy to follow)\n",
    "- 'Experimental Setup or Environment': No (no specific details mentioned in this description)\n",
    "- 'Cannot extract concrete factors that Eased Reproducibility': No (we extracted several factors)\n",
    "\n",
    "This matches the ground truth perfectly! Confidence level: 5/5.  The description is indeed packed with reasons why reproducibility is easy, hitting multiple categories.\n",
    "```\n",
    "</think>\n",
    "<label>\n",
    "[1, 1, 1, 0, 0]\n",
    "</label><|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "test = [a]\n",
    "stepwise_label_reward_func(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "zBYkoXBxTIou"
   },
   "outputs": [],
   "source": [
    "# @title 4.7 Train $\\pi_{LLMSciSci}$ using the $L_{GRPO}$ policy optimization\n",
    "# from trl import GRPOConfig, GRPOTrainer\n",
    "# from transformers import GenerationConfig\n",
    "# from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# # LoRA config\n",
    "# alpha = 4\n",
    "# r = 4\n",
    "# peft_config = LoraConfig(\n",
    "#     lora_alpha=alpha,\n",
    "#     lora_dropout=0.05,\n",
    "#     r=r,\n",
    "#     bias=\"none\",\n",
    "#     target_modules=\"all-linear\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "# # args\n",
    "# training_args = GRPOConfig(\n",
    "#     output_dir=\"llmscisci-GRPO-gemma\", \\\n",
    "#     run_name=\"GRPO/rn-llmscisci-GRPO-gemma\", \\\n",
    "#     num_generations = 4,\n",
    "#     learning_rate = 5e-6,\n",
    "#     max_prompt_length = prompt_length,\n",
    "#     max_completion_length = max_seq_length - prompt_length,\n",
    "#     logging_steps=1, \\\n",
    "#     report_to=[\"wandb\"], \\\n",
    "#     num_train_epochs=1, \\\n",
    "#     max_grad_norm = 20, \\\n",
    "#     label_names=[\"completion\"]\n",
    "# )\n",
    "\n",
    "# # init GRPO trainer\n",
    "# trainer = GRPOTrainer(\n",
    "#     model = model,\n",
    "#     processing_class = tokenizer,\n",
    "#     reward_funcs = [\n",
    "#         format_reward_func,\n",
    "#         label_reward_func,\n",
    "#         stepwise_label_reward_func\n",
    "#     ],\n",
    "#     args = training_args,\n",
    "#     train_dataset = train_dataset,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5DBhZFLGe-N"
   },
   "outputs": [],
   "source": [
    "# train\n",
    "# trainer.train()\n",
    "\n",
    "# save model weights\n",
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0oQDLfnKWEg"
   },
   "source": [
    "### 5. Inference check $\\pi_{LLMSciSci}$ against $\\pi_{LLM-instruct}$ on sample outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zLHFH7rRWMi"
   },
   "outputs": [],
   "source": [
    "# sample inference\n",
    "# @title 5.1 Load reference model for $\\pi_{LLM-Instruct}$\n",
    "# load reference model without policy update\n",
    "# ref_model, _ = load_model(model_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "eQHN6El-KJSQ"
   },
   "outputs": [],
   "source": [
    "# @title 5.2 Sample inference check on $D_{test}$\n",
    "# from IPython.display import display, Latex\n",
    "\n",
    "# # seed for reproducibility\n",
    "# set_seed(2025)\n",
    "\n",
    "# # set top_p and temperature to none\n",
    "# ref_model.generation_config.temperature=None\n",
    "# ref_model.generation_config.top_p=None\n",
    "# trainer.model.generation_config.temperature=None\n",
    "# trainer.model.generation_config.top_p=None\n",
    "\n",
    "# # inputs, attention mask, and shape\n",
    "# idx = 0\n",
    "# input_encoded = tokenizer(test_dataset[\"prompt\"][idx], padding=True, return_tensors=\"pt\")\n",
    "# input_encoded_ids = input_encoded[\"input_ids\"].to(\"cuda\")\n",
    "# input_encoded_attn_mask = input_encoded[\"attention_mask\"].to(\"cuda\")\n",
    "# ref_input_encoded_ids = input_encoded[\"input_ids\"].to(\"cuda\")\n",
    "# ref_input_encoded_attn_mask = input_encoded[\"attention_mask\"].to(\"cuda\")\n",
    "# input_shape = len(input_encoded[\"input_ids\"][0])\n",
    "\n",
    "# # model outputs on test prompts\n",
    "# outputs = trainer.model.generate(\n",
    "#     input_ids=input_encoded_ids,\n",
    "#     attention_mask=input_encoded_attn_mask,\n",
    "#     max_new_tokens=512,\n",
    "#     do_sample=False,\n",
    "#     pad_token_id=tokenizer.pad_token_id,\n",
    "#     eos_token_id=tokenizer.encode(\"<|im_end|>\")\n",
    "# )\n",
    "\n",
    "# # reference model outputs on test prompts\n",
    "# ref_outputs = ref_model.generate(\n",
    "#     input_ids=ref_input_encoded_ids,\n",
    "#     attention_mask=ref_input_encoded_attn_mask,\n",
    "#     max_new_tokens=512,\n",
    "#     do_sample=False,\n",
    "#     pad_token_id=tokenizer.pad_token_id,\n",
    "#     eos_token_id=tokenizer.encode(\"<|im_end|>\")\n",
    "# )\n",
    "\n",
    "# print(\"LLMSciSci-GRPO Policy model response\")\n",
    "# output = tokenizer.decode(outputs[0][input_shape:], skip_special_tokens=True)\n",
    "# print(output)\n",
    "# print(\"------------------------------------------------------\")\n",
    "# print(\"Reference model response (qwen-2.5-1.5b)\")\n",
    "# ref_output = tokenizer.decode(ref_outputs[0][input_shape:], skip_special_tokens=True)\n",
    "# print(ref_output)\n",
    "# print(\"------------------------------------------------------\")\n",
    "# print(\"Correct response:\")\n",
    "# print(test_dataset[\"completion\"][idx])\n",
    "# print(\"------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O91dErD-J_Eq"
   },
   "outputs": [],
   "source": [
    "# del model, tokenizer, trainer\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8frGMZdYLHl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08654c67b28d41b39a1c9c5c755b5bb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e8a92d1818b4275b17482dce8a371c9",
      "placeholder": "​",
      "style": "IPY_MODEL_1aafa4c73cc849feae3779ff7397260b",
      "value": " 46/46 [00:00&lt;00:00, 267.93 examples/s]"
     }
    },
    "0d96a09c7728435182dc7268c090d899": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b273502164ff4d4a8946f3b6c922a1e1",
       "IPY_MODEL_bab1fdd8d53b42feacf8b2669fcb8c83",
       "IPY_MODEL_6768710f6e2647deb3c957ec9e80bb27"
      ],
      "layout": "IPY_MODEL_746a7e56e32147b198aeb507f65f8f0d"
     }
    },
    "14140511d7d3420cb828031090febc4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "15ea96b6ee064ee28b79a39831d692c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1aafa4c73cc849feae3779ff7397260b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "285b6fe358df4fe9af94834e03bb27cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c93f441f71c44e0a164bb3a77db866d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3e8a92d1818b4275b17482dce8a371c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6768710f6e2647deb3c957ec9e80bb27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5897113524741bb88e4e4e64976e90d",
      "placeholder": "​",
      "style": "IPY_MODEL_93babdc2456c4d58b4e2299efe868126",
      "value": " 184/184 [00:00&lt;00:00, 341.50 examples/s]"
     }
    },
    "746a7e56e32147b198aeb507f65f8f0d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bcd57a7f2284330a8daf68025181957": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d0a322daa3b340e8979f758622ae4d92",
       "IPY_MODEL_7bd12161274f4d918f77b538520d7682",
       "IPY_MODEL_08654c67b28d41b39a1c9c5c755b5bb6"
      ],
      "layout": "IPY_MODEL_7f51fd31477648efad201d9680ab2609"
     }
    },
    "7bd12161274f4d918f77b538520d7682": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6132dd3a7e3425eb24191afcbd118a2",
      "max": 46,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c93f441f71c44e0a164bb3a77db866d",
      "value": 46
     }
    },
    "7f51fd31477648efad201d9680ab2609": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "902b10d8fa5d494a87ed28102f17df3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93babdc2456c4d58b4e2299efe868126": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6132dd3a7e3425eb24191afcbd118a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8233b74e41541f781a2d61aa762e24c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b273502164ff4d4a8946f3b6c922a1e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15ea96b6ee064ee28b79a39831d692c5",
      "placeholder": "​",
      "style": "IPY_MODEL_902b10d8fa5d494a87ed28102f17df3f",
      "value": "Filter: 100%"
     }
    },
    "b5897113524741bb88e4e4e64976e90d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bab1fdd8d53b42feacf8b2669fcb8c83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e32c1c277cd14331abfd9ba48924a13d",
      "max": 184,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_14140511d7d3420cb828031090febc4c",
      "value": 184
     }
    },
    "d0a322daa3b340e8979f758622ae4d92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8233b74e41541f781a2d61aa762e24c",
      "placeholder": "​",
      "style": "IPY_MODEL_285b6fe358df4fe9af94834e03bb27cd",
      "value": "Filter: 100%"
     }
    },
    "e32c1c277cd14331abfd9ba48924a13d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
